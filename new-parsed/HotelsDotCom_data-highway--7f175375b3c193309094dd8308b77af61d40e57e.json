[
    {
        "rev_path": "agent/tollbooth/road-tollbooth-app/src/test/java/com/hotels/road/tollbooth/app/TollboothAppIntegrationTest.java",
        "definition": "public class TollboothAppIntegrationTest {\n  private static final String ROAD_TOPIC = \"road\";\n  private static final String PATCH_TOPIC = \"patch\";\n\n  private static final int NUM_BROKERS = 1;\n\n  private static final ObjectMapper mapper = new ObjectMapper();\n\n  @ClassRule\n  public static EmbeddedKafkaCluster kafka = new EmbeddedKafkaCluster(NUM_BROKERS);\n  private static ConfigurableApplicationContext context;\n\n  private static KafkaProducer<String, String> producer;\n\n  private static Map<String, JsonNode> store;\n\n  @BeforeClass\n  public static void beforeClass() throws Exception {\n    int port;\n    try (ServerSocket socket = new ServerSocket(0)) {\n      port = socket.getLocalPort();\n    }\n\n    context = new SpringApplicationBuilder(TollboothApp.class)\n        .bannerMode(OFF)\n        .properties(ImmutableMap\n            .<String, Object> builder()\n            .put(\"server.port\", port)\n            .put(\"kafka.bootstrapServers\", kafka.bootstrapServers())\n            .put(\"kafka.zookeeper\", kafka.zKConnectString())\n            .put(\"kafka.store.topic\", ROAD_TOPIC)\n            .put(\"kafka.store.replicas\", \"1\")\n            .put(\"kafka.patch.topic\", PATCH_TOPIC)\n            .put(\"kafka.patch.replicas\", \"1\")\n            .put(\"kafka.patch.groupId\", \"patches\")\n            .build())\n        .build()\n        .run();\n\n    Properties properties = new Properties();\n    properties.setProperty(\"bootstrap.servers\", kafka.bootstrapServers());\n    producer = new KafkaProducer<>(properties, new StringSerializer(), new StringSerializer());\n\n    store = new KafkaStore<>(kafka.bootstrapServers(), new JsonNodeSerializer(mapper), ROAD_TOPIC);\n  }\n\n  @Test(timeout = 20000)\n  public void create_document_from_patch() throws Exception {\n    producer.send(new ProducerRecord<>(PATCH_TOPIC,\n        \"{\\\"documentId\\\":\\\"road0\\\",\\\"operations\\\":[{\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"\\\",\\\"value\\\":{\\\"name\\\":\\\"hello\\\"}}]}\"));\n\n    while (true) {\n      JsonNode result = store.get(\"road0\");\n      if (result != null) {\n        assertThat(result.path(\"name\").textValue(), is(\"hello\"));\n        return;\n      }\n      Thread.sleep(100);\n    }\n  }\n\n  @Test(timeout = 20000)\n  public void bad_patchs_are_skipped() throws Exception {\n    store.put(\"road1\", mapper.readTree(\"{\\\"name\\\":\\\"hello\\\"}\"));\n\n    producer.send(new ProducerRecord<>(PATCH_TOPIC, \"not json\"));\n    producer.send(new ProducerRecord<>(PATCH_TOPIC,\n        \"{\\\"documentId\\\":\\\"road1\\\",\\\"operations\\\":[{\\\"op\\\":\\\"replace\\\",\\\"path\\\":\\\"/name\\\",\\\"value\\\":\\\"hi there\\\"}]}\"));\n\n    while (!\"hi there\".equals(store.get(\"road1\").path(\"name\").asText())) {\n      Thread.sleep(100);\n    }\n  }\n\n  @Test(timeout = 20000)\n  public void remove_document() throws Exception {\n    store.put(\"road2\", mapper.readTree(\"{\\\"name\\\":\\\"goodbye\\\"}\"));\n\n    producer.send(new ProducerRecord<>(PATCH_TOPIC,\n        \"{\\\"documentId\\\":\\\"road2\\\",\\\"operations\\\":[{\\\"op\\\":\\\"remove\\\",\\\"path\\\":\\\"\\\"}]}\"));\n\n    while (store.containsKey(\"road2\")) {\n      Thread.sleep(100);\n    }\n  }\n\n  @AfterClass\n  public static void after() {\n    if (context != null) {\n      context.close();\n    }\n  }\n}",
        "package": "package com.hotels.road.tollbooth.app;",
        "tree_path": "TollboothAppIntegrationTest",
        "name": "TollboothAppIntegrationTest",
        "modifiers": "public",
        "superclass": null,
        "super_interfaces": null,
        "body": "{\n  private static final String ROAD_TOPIC = \"road\";\n  private static final String PATCH_TOPIC = \"patch\";\n\n  private static final int NUM_BROKERS = 1;\n\n  private static final ObjectMapper mapper = new ObjectMapper();\n\n  @ClassRule\n  public static EmbeddedKafkaCluster kafka = new EmbeddedKafkaCluster(NUM_BROKERS);\n  private static ConfigurableApplicationContext context;\n\n  private static KafkaProducer<String, String> producer;\n\n  private static Map<String, JsonNode> store;\n\n  @BeforeClass\n  public static void beforeClass() throws Exception {\n    int port;\n    try (ServerSocket socket = new ServerSocket(0)) {\n      port = socket.getLocalPort();\n    }\n\n    context = new SpringApplicationBuilder(TollboothApp.class)\n        .bannerMode(OFF)\n        .properties(ImmutableMap\n            .<String, Object> builder()\n            .put(\"server.port\", port)\n            .put(\"kafka.bootstrapServers\", kafka.bootstrapServers())\n            .put(\"kafka.zookeeper\", kafka.zKConnectString())\n            .put(\"kafka.store.topic\", ROAD_TOPIC)\n            .put(\"kafka.store.replicas\", \"1\")\n            .put(\"kafka.patch.topic\", PATCH_TOPIC)\n            .put(\"kafka.patch.replicas\", \"1\")\n            .put(\"kafka.patch.groupId\", \"patches\")\n            .build())\n        .build()\n        .run();\n\n    Properties properties = new Properties();\n    properties.setProperty(\"bootstrap.servers\", kafka.bootstrapServers());\n    producer = new KafkaProducer<>(properties, new StringSerializer(), new StringSerializer());\n\n    store = new KafkaStore<>(kafka.bootstrapServers(), new JsonNodeSerializer(mapper), ROAD_TOPIC);\n  }\n\n  @Test(timeout = 20000)\n  public void create_document_from_patch() throws Exception {\n    producer.send(new ProducerRecord<>(PATCH_TOPIC,\n        \"{\\\"documentId\\\":\\\"road0\\\",\\\"operations\\\":[{\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"\\\",\\\"value\\\":{\\\"name\\\":\\\"hello\\\"}}]}\"));\n\n    while (true) {\n      JsonNode result = store.get(\"road0\");\n      if (result != null) {\n        assertThat(result.path(\"name\").textValue(), is(\"hello\"));\n        return;\n      }\n      Thread.sleep(100);\n    }\n  }\n\n  @Test(timeout = 20000)\n  public void bad_patchs_are_skipped() throws Exception {\n    store.put(\"road1\", mapper.readTree(\"{\\\"name\\\":\\\"hello\\\"}\"));\n\n    producer.send(new ProducerRecord<>(PATCH_TOPIC, \"not json\"));\n    producer.send(new ProducerRecord<>(PATCH_TOPIC,\n        \"{\\\"documentId\\\":\\\"road1\\\",\\\"operations\\\":[{\\\"op\\\":\\\"replace\\\",\\\"path\\\":\\\"/name\\\",\\\"value\\\":\\\"hi there\\\"}]}\"));\n\n    while (!\"hi there\".equals(store.get(\"road1\").path(\"name\").asText())) {\n      Thread.sleep(100);\n    }\n  }\n\n  @Test(timeout = 20000)\n  public void remove_document() throws Exception {\n    store.put(\"road2\", mapper.readTree(\"{\\\"name\\\":\\\"goodbye\\\"}\"));\n\n    producer.send(new ProducerRecord<>(PATCH_TOPIC,\n        \"{\\\"documentId\\\":\\\"road2\\\",\\\"operations\\\":[{\\\"op\\\":\\\"remove\\\",\\\"path\\\":\\\"\\\"}]}\"));\n\n    while (store.containsKey(\"road2\")) {\n      Thread.sleep(100);\n    }\n  }\n\n  @AfterClass\n  public static void after() {\n    if (context != null) {\n      context.close();\n    }\n  }\n}",
        "start_point": {
            "row": 42,
            "column": 0
        },
        "end_point": {
            "row": 134,
            "column": 1
        },
        "file_mode": "Modified",
        "map_path": null,
        "methods": [
            {
                "definition": "@BeforeClass\npublic static void beforeClass() throws Exception {\n  int port;\n  try (ServerSocket socket = new ServerSocket(0)) {\n    port = socket.getLocalPort();\n  }\n\n  context = new SpringApplicationBuilder(TollboothApp.class)\n      .bannerMode(OFF)\n      .properties(ImmutableMap\n          .<String, Object> builder()\n          .put(\"server.port\", port)\n          .put(\"kafka.bootstrapServers\", kafka.bootstrapServers())\n          .put(\"kafka.zookeeper\", kafka.zKConnectString())\n          .put(\"kafka.store.topic\", ROAD_TOPIC)\n          .put(\"kafka.store.replicas\", \"1\")\n          .put(\"kafka.patch.topic\", PATCH_TOPIC)\n          .put(\"kafka.patch.replicas\", \"1\")\n          .put(\"kafka.patch.groupId\", \"patches\")\n          .build())\n      .build()\n      .run();\n\n  Properties properties = new Properties();\n  properties.setProperty(\"bootstrap.servers\", kafka.bootstrapServers());\n  producer = new KafkaProducer<>(properties, new StringSerializer(), new StringSerializer());\n\n  store = new KafkaStore<>(kafka.bootstrapServers(), new JsonNodeSerializer(mapper), ROAD_TOPIC);\n}",
                "name": "beforeClass",
                "modifiers": "@BeforeClass\n  public static",
                "return_type": null,
                "parameters": [],
                "body": "{\n  int port;\n  try (ServerSocket socket = new ServerSocket(0)) {\n    port = socket.getLocalPort();\n  }\n\n  context = new SpringApplicationBuilder(TollboothApp.class)\n      .bannerMode(OFF)\n      .properties(ImmutableMap\n          .<String, Object> builder()\n          .put(\"server.port\", port)\n          .put(\"kafka.bootstrapServers\", kafka.bootstrapServers())\n          .put(\"kafka.zookeeper\", kafka.zKConnectString())\n          .put(\"kafka.store.topic\", ROAD_TOPIC)\n          .put(\"kafka.store.replicas\", \"1\")\n          .put(\"kafka.patch.topic\", PATCH_TOPIC)\n          .put(\"kafka.patch.replicas\", \"1\")\n          .put(\"kafka.patch.groupId\", \"patches\")\n          .build())\n      .build()\n      .run();\n\n  Properties properties = new Properties();\n  properties.setProperty(\"bootstrap.servers\", kafka.bootstrapServers());\n  producer = new KafkaProducer<>(properties, new StringSerializer(), new StringSerializer());\n\n  store = new KafkaStore<>(kafka.bootstrapServers(), new JsonNodeSerializer(mapper), ROAD_TOPIC);\n}",
                "start_point": {
                    "row": 58,
                    "column": 2
                },
                "end_point": {
                    "row": 86,
                    "column": 3
                }
            },
            {
                "definition": "@Test(timeout = 20000)\npublic void create_document_from_patch() throws Exception {\n  producer.send(new ProducerRecord<>(PATCH_TOPIC,\n      \"{\\\"documentId\\\":\\\"road0\\\",\\\"operations\\\":[{\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"\\\",\\\"value\\\":{\\\"name\\\":\\\"hello\\\"}}]}\"));\n\n  while (true) {\n    JsonNode result = store.get(\"road0\");\n    if (result != null) {\n      assertThat(result.path(\"name\").textValue(), is(\"hello\"));\n      return;\n    }\n    Thread.sleep(100);\n  }\n}",
                "name": "create_document_from_patch",
                "modifiers": "@Test(timeout = 20000)\n  public",
                "return_type": null,
                "parameters": [],
                "body": "{\n  producer.send(new ProducerRecord<>(PATCH_TOPIC,\n      \"{\\\"documentId\\\":\\\"road0\\\",\\\"operations\\\":[{\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"\\\",\\\"value\\\":{\\\"name\\\":\\\"hello\\\"}}]}\"));\n\n  while (true) {\n    JsonNode result = store.get(\"road0\");\n    if (result != null) {\n      assertThat(result.path(\"name\").textValue(), is(\"hello\"));\n      return;\n    }\n    Thread.sleep(100);\n  }\n}",
                "start_point": {
                    "row": 88,
                    "column": 2
                },
                "end_point": {
                    "row": 101,
                    "column": 3
                }
            },
            {
                "definition": "@Test(timeout = 20000)\npublic void bad_patchs_are_skipped() throws Exception {\n  store.put(\"road1\", mapper.readTree(\"{\\\"name\\\":\\\"hello\\\"}\"));\n\n  producer.send(new ProducerRecord<>(PATCH_TOPIC, \"not json\"));\n  producer.send(new ProducerRecord<>(PATCH_TOPIC,\n      \"{\\\"documentId\\\":\\\"road1\\\",\\\"operations\\\":[{\\\"op\\\":\\\"replace\\\",\\\"path\\\":\\\"/name\\\",\\\"value\\\":\\\"hi there\\\"}]}\"));\n\n  while (!\"hi there\".equals(store.get(\"road1\").path(\"name\").asText())) {\n    Thread.sleep(100);\n  }\n}",
                "name": "bad_patchs_are_skipped",
                "modifiers": "@Test(timeout = 20000)\n  public",
                "return_type": null,
                "parameters": [],
                "body": "{\n  store.put(\"road1\", mapper.readTree(\"{\\\"name\\\":\\\"hello\\\"}\"));\n\n  producer.send(new ProducerRecord<>(PATCH_TOPIC, \"not json\"));\n  producer.send(new ProducerRecord<>(PATCH_TOPIC,\n      \"{\\\"documentId\\\":\\\"road1\\\",\\\"operations\\\":[{\\\"op\\\":\\\"replace\\\",\\\"path\\\":\\\"/name\\\",\\\"value\\\":\\\"hi there\\\"}]}\"));\n\n  while (!\"hi there\".equals(store.get(\"road1\").path(\"name\").asText())) {\n    Thread.sleep(100);\n  }\n}",
                "start_point": {
                    "row": 103,
                    "column": 2
                },
                "end_point": {
                    "row": 114,
                    "column": 3
                }
            },
            {
                "definition": "@Test(timeout = 20000)\npublic void remove_document() throws Exception {\n  store.put(\"road2\", mapper.readTree(\"{\\\"name\\\":\\\"goodbye\\\"}\"));\n\n  producer.send(new ProducerRecord<>(PATCH_TOPIC,\n      \"{\\\"documentId\\\":\\\"road2\\\",\\\"operations\\\":[{\\\"op\\\":\\\"remove\\\",\\\"path\\\":\\\"\\\"}]}\"));\n\n  while (store.containsKey(\"road2\")) {\n    Thread.sleep(100);\n  }\n}",
                "name": "remove_document",
                "modifiers": "@Test(timeout = 20000)\n  public",
                "return_type": null,
                "parameters": [],
                "body": "{\n  store.put(\"road2\", mapper.readTree(\"{\\\"name\\\":\\\"goodbye\\\"}\"));\n\n  producer.send(new ProducerRecord<>(PATCH_TOPIC,\n      \"{\\\"documentId\\\":\\\"road2\\\",\\\"operations\\\":[{\\\"op\\\":\\\"remove\\\",\\\"path\\\":\\\"\\\"}]}\"));\n\n  while (store.containsKey(\"road2\")) {\n    Thread.sleep(100);\n  }\n}",
                "start_point": {
                    "row": 116,
                    "column": 2
                },
                "end_point": {
                    "row": 126,
                    "column": 3
                }
            },
            {
                "definition": "@AfterClass\npublic static void after() {\n  if (context != null) {\n    context.close();\n  }\n}",
                "name": "after",
                "modifiers": "@AfterClass\n  public static",
                "return_type": null,
                "parameters": [],
                "body": "{\n  if (context != null) {\n    context.close();\n  }\n}",
                "start_point": {
                    "row": 128,
                    "column": 2
                },
                "end_point": {
                    "row": 133,
                    "column": 3
                }
            }
        ]
    },
    {
        "rev_path": "monitoring/kafka-offset-metrics/src/main/java/com/hotels/road/kafka/offset/metrics/KafkaOffsetMetrics.java",
        "definition": "@Component\n@RequiredArgsConstructor\n@Slf4j\nclass KafkaOffsetMetrics {\n  private final AdminClient adminClient;\n  private final ScheduledReporter reporter;\n\n  @Scheduled(initialDelayString = \"${metricRate:60000}\", fixedRateString = \"${metricRate:60000}\")\n  void sendOffsets() {\n    log.info(\"Sending offsets\");\n    // We're calling listAllGroupsFlattened as opposed to listAllConsumerGroupsFlattened because we're not\n    // consuming data with the KafkaConsumer in hive-agent (it doesn't subscribe/poll), so when it commits the\n    // protocolType is null when it would otherwise be 'consumer'.\n    @SuppressWarnings(\"rawtypes\")\n    SortedMap<String, Gauge> gauges = asJavaCollection(adminClient.listAllGroupsFlattened())\n        .stream()\n        .map(GroupOverview::groupId)\n        .flatMap(groupId -> {\n          log.info(\"Found groupId: {}\", groupId);\n          return mapAsJavaMap(adminClient.listGroupOffsets(groupId)).entrySet().stream().map(\n              entry -> createMetric(groupId, entry));\n        })\n        .collect(toMap(Entry::getKey, Entry::getValue, (a, b) -> a, () -> new TreeMap<>()));\n\n    reporter.report(gauges, emptySortedMap(), emptySortedMap(), emptySortedMap(), emptySortedMap());\n    log.info(\"Done\");\n  }\n\n  @SuppressWarnings(\"rawtypes\")\n  private Entry<String, Gauge> createMetric(String groupId, Entry<TopicPartition, Object> entry) {\n    return Maps.immutableEntry(metricName(groupId, entry.getKey()), (Gauge) () -> entry.getValue());\n  }\n\n  private String metricName(String groupId, TopicPartition topicPartition) {\n    return MetricRegistry.name(\"group\", clean(groupId), \"topic\", clean(topicPartition.topic()), \"partition\",\n        Integer.toString(topicPartition.partition()), \"offset\");\n  }\n\n  private String clean(String name) {\n    return name.replaceAll(\"\\\\.\", \"_\");\n  }\n\n}",
        "package": "package com.hotels.road.kafka.offset.metrics;",
        "tree_path": "KafkaOffsetMetrics",
        "name": "KafkaOffsetMetrics",
        "modifiers": "@Component\n@RequiredArgsConstructor\n@Slf4j",
        "superclass": null,
        "super_interfaces": null,
        "body": "{\n  private final AdminClient adminClient;\n  private final ScheduledReporter reporter;\n\n  @Scheduled(initialDelayString = \"${metricRate:60000}\", fixedRateString = \"${metricRate:60000}\")\n  void sendOffsets() {\n    log.info(\"Sending offsets\");\n    // We're calling listAllGroupsFlattened as opposed to listAllConsumerGroupsFlattened because we're not\n    // consuming data with the KafkaConsumer in hive-agent (it doesn't subscribe/poll), so when it commits the\n    // protocolType is null when it would otherwise be 'consumer'.\n    @SuppressWarnings(\"rawtypes\")\n    SortedMap<String, Gauge> gauges = asJavaCollection(adminClient.listAllGroupsFlattened())\n        .stream()\n        .map(GroupOverview::groupId)\n        .flatMap(groupId -> {\n          log.info(\"Found groupId: {}\", groupId);\n          return mapAsJavaMap(adminClient.listGroupOffsets(groupId)).entrySet().stream().map(\n              entry -> createMetric(groupId, entry));\n        })\n        .collect(toMap(Entry::getKey, Entry::getValue, (a, b) -> a, () -> new TreeMap<>()));\n\n    reporter.report(gauges, emptySortedMap(), emptySortedMap(), emptySortedMap(), emptySortedMap());\n    log.info(\"Done\");\n  }\n\n  @SuppressWarnings(\"rawtypes\")\n  private Entry<String, Gauge> createMetric(String groupId, Entry<TopicPartition, Object> entry) {\n    return Maps.immutableEntry(metricName(groupId, entry.getKey()), (Gauge) () -> entry.getValue());\n  }\n\n  private String metricName(String groupId, TopicPartition topicPartition) {\n    return MetricRegistry.name(\"group\", clean(groupId), \"topic\", clean(topicPartition.topic()), \"partition\",\n        Integer.toString(topicPartition.partition()), \"offset\");\n  }\n\n  private String clean(String name) {\n    return name.replaceAll(\"\\\\.\", \"_\");\n  }\n\n}",
        "start_point": {
            "row": 41,
            "column": 0
        },
        "end_point": {
            "row": 83,
            "column": 1
        },
        "file_mode": "Modified",
        "map_path": null,
        "methods": [
            {
                "definition": "@Scheduled(initialDelayString = \"${metricRate:60000}\", fixedRateString = \"${metricRate:60000}\")\nvoid sendOffsets() {\n  log.info(\"Sending offsets\");\n  // We're calling listAllGroupsFlattened as opposed to listAllConsumerGroupsFlattened because we're not\n  // consuming data with the KafkaConsumer in hive-agent (it doesn't subscribe/poll), so when it commits the\n  // protocolType is null when it would otherwise be 'consumer'.\n  @SuppressWarnings(\"rawtypes\")\n  SortedMap<String, Gauge> gauges = asJavaCollection(adminClient.listAllGroupsFlattened())\n      .stream()\n      .map(GroupOverview::groupId)\n      .flatMap(groupId -> {\n        log.info(\"Found groupId: {}\", groupId);\n        return mapAsJavaMap(adminClient.listGroupOffsets(groupId)).entrySet().stream().map(\n            entry -> createMetric(groupId, entry));\n      })\n      .collect(toMap(Entry::getKey, Entry::getValue, (a, b) -> a, () -> new TreeMap<>()));\n\n  reporter.report(gauges, emptySortedMap(), emptySortedMap(), emptySortedMap(), emptySortedMap());\n  log.info(\"Done\");\n}",
                "name": "sendOffsets",
                "modifiers": "@Scheduled(initialDelayString = \"${metricRate:60000}\", fixedRateString = \"${metricRate:60000}\")",
                "return_type": null,
                "parameters": [],
                "body": "{\n  log.info(\"Sending offsets\");\n  // We're calling listAllGroupsFlattened as opposed to listAllConsumerGroupsFlattened because we're not\n  // consuming data with the KafkaConsumer in hive-agent (it doesn't subscribe/poll), so when it commits the\n  // protocolType is null when it would otherwise be 'consumer'.\n  @SuppressWarnings(\"rawtypes\")\n  SortedMap<String, Gauge> gauges = asJavaCollection(adminClient.listAllGroupsFlattened())\n      .stream()\n      .map(GroupOverview::groupId)\n      .flatMap(groupId -> {\n        log.info(\"Found groupId: {}\", groupId);\n        return mapAsJavaMap(adminClient.listGroupOffsets(groupId)).entrySet().stream().map(\n            entry -> createMetric(groupId, entry));\n      })\n      .collect(toMap(Entry::getKey, Entry::getValue, (a, b) -> a, () -> new TreeMap<>()));\n\n  reporter.report(gauges, emptySortedMap(), emptySortedMap(), emptySortedMap(), emptySortedMap());\n  log.info(\"Done\");\n}",
                "start_point": {
                    "row": 48,
                    "column": 2
                },
                "end_point": {
                    "row": 67,
                    "column": 3
                }
            },
            {
                "definition": "@SuppressWarnings(\"rawtypes\")\nprivate Entry<String, Gauge> createMetric(String groupId, Entry<TopicPartition, Object> entry) {\n  return Maps.immutableEntry(metricName(groupId, entry.getKey()), (Gauge) () -> entry.getValue());\n}",
                "name": "createMetric",
                "modifiers": "@SuppressWarnings(\"rawtypes\")\n  private",
                "return_type": null,
                "parameters": [
                    {
                        "type": "String",
                        "name": "groupId"
                    },
                    {
                        "type": "Entry<TopicPartition, Object>",
                        "name": "entry"
                    }
                ],
                "body": "{\n  return Maps.immutableEntry(metricName(groupId, entry.getKey()), (Gauge) () -> entry.getValue());\n}",
                "start_point": {
                    "row": 69,
                    "column": 2
                },
                "end_point": {
                    "row": 72,
                    "column": 3
                }
            },
            {
                "definition": "private String metricName(String groupId, TopicPartition topicPartition) {\n  return MetricRegistry.name(\"group\", clean(groupId), \"topic\", clean(topicPartition.topic()), \"partition\",\n      Integer.toString(topicPartition.partition()), \"offset\");\n}",
                "name": "metricName",
                "modifiers": "private",
                "return_type": "String",
                "parameters": [
                    {
                        "type": "String",
                        "name": "groupId"
                    },
                    {
                        "type": "TopicPartition",
                        "name": "topicPartition"
                    }
                ],
                "body": "{\n  return MetricRegistry.name(\"group\", clean(groupId), \"topic\", clean(topicPartition.topic()), \"partition\",\n      Integer.toString(topicPartition.partition()), \"offset\");\n}",
                "start_point": {
                    "row": 74,
                    "column": 2
                },
                "end_point": {
                    "row": 77,
                    "column": 3
                }
            },
            {
                "definition": "private String clean(String name) {\n  return name.replaceAll(\"\\\\.\", \"_\");\n}",
                "name": "clean",
                "modifiers": "private",
                "return_type": "String",
                "parameters": [
                    {
                        "type": "String",
                        "name": "name"
                    }
                ],
                "body": "{\n  return name.replaceAll(\"\\\\.\", \"_\");\n}",
                "start_point": {
                    "row": 79,
                    "column": 2
                },
                "end_point": {
                    "row": 81,
                    "column": 3
                }
            }
        ]
    },
    {
        "rev_path": "monitoring/kafka-offset-metrics/src/main/java/com/hotels/road/kafka/offset/metrics/KafkaOffsetMetricsApp.java",
        "definition": "@SpringBootApplication\n@EnableScheduling\npublic class KafkaOffsetMetricsApp {\n  @Bean\n  AdminClient adminClient(@Value(\"${kafka.bootstrapServers}\") String bootstrapServers) {\n    Properties properties = new Properties();\n    properties.setProperty(\"bootstrap.servers\", bootstrapServers);\n    return AdminClient.create(properties);\n  }\n\n  @Bean\n  ScheduledReporter reporter(\n      @Value(\"${graphite.endpoint}\") String graphiteEndpoint,\n      @Value(\"${graphite.prefix:road}\") String graphitePrefix,\n      Clock clock,\n      Supplier<String> hostnameSupplier) {\n    HostAndPort hostAndPort = HostAndPort.fromString(graphiteEndpoint);\n    InetSocketAddress socketAddress = new InetSocketAddress(hostAndPort.getHost(), hostAndPort.getPort());\n    return GraphiteReporter\n        .forRegistry(new MetricRegistry())\n        .prefixedWith(MetricRegistry.name(graphitePrefix, \"kafka-offset\", \"host\", hostnameSupplier.get()))\n        .withClock(clock)\n        .build(new Graphite(socketAddress));\n  }\n\n  @Bean\n  Clock clock() {\n    return Clock.defaultClock();\n  }\n\n  @Bean\n  Supplier<String> hostnameSupplier() {\n    return () -> {\n      try {\n        return InetAddress.getLocalHost().getHostName();\n      } catch (UnknownHostException e) {\n        throw new RuntimeException(e);\n      }\n    };\n  }\n\n  public static void main(String[] args) {\n    DataHighwayApplication.run(KafkaOffsetMetricsApp.class, args);\n  }\n}",
        "package": "package com.hotels.road.kafka.offset.metrics;",
        "tree_path": "KafkaOffsetMetricsApp",
        "name": "KafkaOffsetMetricsApp",
        "modifiers": "@SpringBootApplication\n@EnableScheduling\npublic",
        "superclass": null,
        "super_interfaces": null,
        "body": "{\n  @Bean\n  AdminClient adminClient(@Value(\"${kafka.bootstrapServers}\") String bootstrapServers) {\n    Properties properties = new Properties();\n    properties.setProperty(\"bootstrap.servers\", bootstrapServers);\n    return AdminClient.create(properties);\n  }\n\n  @Bean\n  ScheduledReporter reporter(\n      @Value(\"${graphite.endpoint}\") String graphiteEndpoint,\n      @Value(\"${graphite.prefix:road}\") String graphitePrefix,\n      Clock clock,\n      Supplier<String> hostnameSupplier) {\n    HostAndPort hostAndPort = HostAndPort.fromString(graphiteEndpoint);\n    InetSocketAddress socketAddress = new InetSocketAddress(hostAndPort.getHost(), hostAndPort.getPort());\n    return GraphiteReporter\n        .forRegistry(new MetricRegistry())\n        .prefixedWith(MetricRegistry.name(graphitePrefix, \"kafka-offset\", \"host\", hostnameSupplier.get()))\n        .withClock(clock)\n        .build(new Graphite(socketAddress));\n  }\n\n  @Bean\n  Clock clock() {\n    return Clock.defaultClock();\n  }\n\n  @Bean\n  Supplier<String> hostnameSupplier() {\n    return () -> {\n      try {\n        return InetAddress.getLocalHost().getHostName();\n      } catch (UnknownHostException e) {\n        throw new RuntimeException(e);\n      }\n    };\n  }\n\n  public static void main(String[] args) {\n    DataHighwayApplication.run(KafkaOffsetMetricsApp.class, args);\n  }\n}",
        "start_point": {
            "row": 39,
            "column": 0
        },
        "end_point": {
            "row": 83,
            "column": 1
        },
        "file_mode": "Modified",
        "map_path": null,
        "methods": [
            {
                "definition": "@Bean\nAdminClient adminClient(@Value(\"${kafka.bootstrapServers}\") String bootstrapServers) {\n  Properties properties = new Properties();\n  properties.setProperty(\"bootstrap.servers\", bootstrapServers);\n  return AdminClient.create(properties);\n}",
                "name": "adminClient",
                "modifiers": "@Bean",
                "return_type": "AdminClient",
                "parameters": [
                    {
                        "type": "@Value(\"${kafka.bootstrapServers}\")",
                        "name": "String"
                    }
                ],
                "body": "{\n  Properties properties = new Properties();\n  properties.setProperty(\"bootstrap.servers\", bootstrapServers);\n  return AdminClient.create(properties);\n}",
                "start_point": {
                    "row": 42,
                    "column": 2
                },
                "end_point": {
                    "row": 47,
                    "column": 3
                }
            },
            {
                "definition": "@Bean\nScheduledReporter reporter(\n    @Value(\"${graphite.endpoint}\") String graphiteEndpoint,\n    @Value(\"${graphite.prefix:road}\") String graphitePrefix,\n    Clock clock,\n    Supplier<String> hostnameSupplier) {\n  HostAndPort hostAndPort = HostAndPort.fromString(graphiteEndpoint);\n  InetSocketAddress socketAddress = new InetSocketAddress(hostAndPort.getHost(), hostAndPort.getPort());\n  return GraphiteReporter\n      .forRegistry(new MetricRegistry())\n      .prefixedWith(MetricRegistry.name(graphitePrefix, \"kafka-offset\", \"host\", hostnameSupplier.get()))\n      .withClock(clock)\n      .build(new Graphite(socketAddress));\n}",
                "name": "reporter",
                "modifiers": "@Bean",
                "return_type": "ScheduledReporter",
                "parameters": [
                    {
                        "type": "@Value(\"${graphite.endpoint}\")",
                        "name": "String"
                    },
                    {
                        "type": "@Value(\"${graphite.prefix:road}\")",
                        "name": "String"
                    },
                    {
                        "type": "Clock",
                        "name": "clock"
                    },
                    {
                        "type": "Supplier<String>",
                        "name": "hostnameSupplier"
                    }
                ],
                "body": "{\n  HostAndPort hostAndPort = HostAndPort.fromString(graphiteEndpoint);\n  InetSocketAddress socketAddress = new InetSocketAddress(hostAndPort.getHost(), hostAndPort.getPort());\n  return GraphiteReporter\n      .forRegistry(new MetricRegistry())\n      .prefixedWith(MetricRegistry.name(graphitePrefix, \"kafka-offset\", \"host\", hostnameSupplier.get()))\n      .withClock(clock)\n      .build(new Graphite(socketAddress));\n}",
                "start_point": {
                    "row": 49,
                    "column": 2
                },
                "end_point": {
                    "row": 62,
                    "column": 3
                }
            },
            {
                "definition": "@Bean\nClock clock() {\n  return Clock.defaultClock();\n}",
                "name": "clock",
                "modifiers": "@Bean",
                "return_type": "Clock",
                "parameters": [],
                "body": "{\n  return Clock.defaultClock();\n}",
                "start_point": {
                    "row": 64,
                    "column": 2
                },
                "end_point": {
                    "row": 67,
                    "column": 3
                }
            },
            {
                "definition": "@Bean\nSupplier<String> hostnameSupplier() {\n  return () -> {\n    try {\n      return InetAddress.getLocalHost().getHostName();\n    } catch (UnknownHostException e) {\n      throw new RuntimeException(e);\n    }\n  };\n}",
                "name": "hostnameSupplier",
                "modifiers": "@Bean",
                "return_type": null,
                "parameters": [],
                "body": "{\n  return () -> {\n    try {\n      return InetAddress.getLocalHost().getHostName();\n    } catch (UnknownHostException e) {\n      throw new RuntimeException(e);\n    }\n  };\n}",
                "start_point": {
                    "row": 69,
                    "column": 2
                },
                "end_point": {
                    "row": 78,
                    "column": 3
                }
            },
            {
                "definition": "public static void main(String[] args) {\n  DataHighwayApplication.run(KafkaOffsetMetricsApp.class, args);\n}",
                "name": "main",
                "modifiers": "public static",
                "return_type": null,
                "parameters": [
                    {
                        "type": "String[]",
                        "name": "args"
                    }
                ],
                "body": "{\n  DataHighwayApplication.run(KafkaOffsetMetricsApp.class, args);\n}",
                "start_point": {
                    "row": 80,
                    "column": 2
                },
                "end_point": {
                    "row": 82,
                    "column": 3
                }
            }
        ]
    },
    {
        "rev_path": "monitoring/kafka-offset-metrics/src/test/java/com/hotels/road/kafka/offset/metrics/KafkaOffsetMetricsAppTest.java",
        "definition": "public class KafkaOffsetMetricsAppTest {\n\n  private static final String TOPIC = \"test.topic\";\n\n  @Rule\n  public EmbeddedKafkaCluster kafka = new EmbeddedKafkaCluster(1);\n\n  @Configuration\n  static class TestConfig {\n    @Primary\n    @Bean\n    Clock clock() {\n      return new Clock() {\n        @Override\n        public long getTick() {\n          return 0L;\n        }\n\n        @Override\n        public long getTime() {\n          return 123000L;\n        }\n      };\n    }\n\n    @Primary\n    @Bean\n    Supplier<String> hostnameSupplier() {\n      return () -> \"hostname\";\n    }\n  }\n\n  @Test\n  public void test() throws Exception {\n    kafka.createTopic(TOPIC);\n\n    try (KafkaConsumer<String, String> consumer = consumer()) {\n      consumer.commitSync(singletonMap(new TopicPartition(TOPIC, 0), new OffsetAndMetadata(1L)));\n    }\n\n    try (ServerSocket serverSocket = new ServerSocket(0)) {\n      CompletableFuture<String> future = CompletableFuture.supplyAsync(() -> {\n        try (Socket socket = serverSocket.accept();\n            BufferedReader reader = new BufferedReader(new InputStreamReader(socket.getInputStream()))) {\n          return reader.readLine();\n        } catch (Exception e) {\n          throw new RuntimeException(e);\n        }\n      });\n\n      try (ConfigurableApplicationContext context = runApp(serverSocket.getLocalPort())) {\n        Awaitility.await().atMost(5, SECONDS).pollInterval(100, MILLISECONDS).until(() -> {\n          assertThat(future.isDone(), is(true));\n          assertThat(future.join(),\n              is(\"road.kafka-offset.host.hostname.group.group_id.topic.test_topic.partition.0.offset 1 123\"));\n        });\n      }\n    }\n  }\n\n  private ConfigurableApplicationContext runApp(int port) {\n    String[] args = ImmutableMap\n        .<String, String> builder()\n        .put(\"kafka.bootstrapServers\", kafka.bootstrapServers())\n        .put(\"graphite.endpoint\", \"localhost:\" + port)\n        .put(\"metricRate\", \"1000\")\n        .build()\n        .entrySet()\n        .stream()\n        .map(e -> String.format(\"--%s=%s\", e.getKey(), e.getValue()))\n        .toArray(i -> new String[i]);\n    return new SpringApplicationBuilder(KafkaOffsetMetricsApp.class, TestConfig.class).bannerMode(OFF).run(args);\n  }\n\n  private KafkaConsumer<String, String> consumer() {\n    Map<String, Object> config = ImmutableMap\n        .<String, Object> builder()\n        .put(\"bootstrap.servers\", kafka.bootstrapServers())\n        .put(\"group.id\", \"group.id\")\n        .put(\"enable.auto.commit\", \"false\")\n        .put(\"auto.offset.reset\", \"earliest\")\n        .build();\n    Deserializer<String> deserializer = new StringDeserializer();\n    return new KafkaConsumer<>(config, deserializer, deserializer);\n  }\n}",
        "package": "package com.hotels.road.kafka.offset.metrics;",
        "tree_path": "KafkaOffsetMetricsAppTest",
        "name": "KafkaOffsetMetricsAppTest",
        "modifiers": "public",
        "superclass": null,
        "super_interfaces": null,
        "body": "{\n\n  private static final String TOPIC = \"test.topic\";\n\n  @Rule\n  public EmbeddedKafkaCluster kafka = new EmbeddedKafkaCluster(1);\n\n  @Configuration\n  static class TestConfig {\n    @Primary\n    @Bean\n    Clock clock() {\n      return new Clock() {\n        @Override\n        public long getTick() {\n          return 0L;\n        }\n\n        @Override\n        public long getTime() {\n          return 123000L;\n        }\n      };\n    }\n\n    @Primary\n    @Bean\n    Supplier<String> hostnameSupplier() {\n      return () -> \"hostname\";\n    }\n  }\n\n  @Test\n  public void test() throws Exception {\n    kafka.createTopic(TOPIC);\n\n    try (KafkaConsumer<String, String> consumer = consumer()) {\n      consumer.commitSync(singletonMap(new TopicPartition(TOPIC, 0), new OffsetAndMetadata(1L)));\n    }\n\n    try (ServerSocket serverSocket = new ServerSocket(0)) {\n      CompletableFuture<String> future = CompletableFuture.supplyAsync(() -> {\n        try (Socket socket = serverSocket.accept();\n            BufferedReader reader = new BufferedReader(new InputStreamReader(socket.getInputStream()))) {\n          return reader.readLine();\n        } catch (Exception e) {\n          throw new RuntimeException(e);\n        }\n      });\n\n      try (ConfigurableApplicationContext context = runApp(serverSocket.getLocalPort())) {\n        Awaitility.await().atMost(5, SECONDS).pollInterval(100, MILLISECONDS).until(() -> {\n          assertThat(future.isDone(), is(true));\n          assertThat(future.join(),\n              is(\"road.kafka-offset.host.hostname.group.group_id.topic.test_topic.partition.0.offset 1 123\"));\n        });\n      }\n    }\n  }\n\n  private ConfigurableApplicationContext runApp(int port) {\n    String[] args = ImmutableMap\n        .<String, String> builder()\n        .put(\"kafka.bootstrapServers\", kafka.bootstrapServers())\n        .put(\"graphite.endpoint\", \"localhost:\" + port)\n        .put(\"metricRate\", \"1000\")\n        .build()\n        .entrySet()\n        .stream()\n        .map(e -> String.format(\"--%s=%s\", e.getKey(), e.getValue()))\n        .toArray(i -> new String[i]);\n    return new SpringApplicationBuilder(KafkaOffsetMetricsApp.class, TestConfig.class).bannerMode(OFF).run(args);\n  }\n\n  private KafkaConsumer<String, String> consumer() {\n    Map<String, Object> config = ImmutableMap\n        .<String, Object> builder()\n        .put(\"bootstrap.servers\", kafka.bootstrapServers())\n        .put(\"group.id\", \"group.id\")\n        .put(\"enable.auto.commit\", \"false\")\n        .put(\"auto.offset.reset\", \"earliest\")\n        .build();\n    Deserializer<String> deserializer = new StringDeserializer();\n    return new KafkaConsumer<>(config, deserializer, deserializer);\n  }\n}",
        "start_point": {
            "row": 51,
            "column": 0
        },
        "end_point": {
            "row": 136,
            "column": 1
        },
        "file_mode": "Modified",
        "map_path": null,
        "methods": [
            {
                "definition": "@Test\npublic void test() throws Exception {\n  kafka.createTopic(TOPIC);\n\n  try (KafkaConsumer<String, String> consumer = consumer()) {\n    consumer.commitSync(singletonMap(new TopicPartition(TOPIC, 0), new OffsetAndMetadata(1L)));\n  }\n\n  try (ServerSocket serverSocket = new ServerSocket(0)) {\n    CompletableFuture<String> future = CompletableFuture.supplyAsync(() -> {\n      try (Socket socket = serverSocket.accept();\n          BufferedReader reader = new BufferedReader(new InputStreamReader(socket.getInputStream()))) {\n        return reader.readLine();\n      } catch (Exception e) {\n        throw new RuntimeException(e);\n      }\n    });\n\n    try (ConfigurableApplicationContext context = runApp(serverSocket.getLocalPort())) {\n      Awaitility.await().atMost(5, SECONDS).pollInterval(100, MILLISECONDS).until(() -> {\n        assertThat(future.isDone(), is(true));\n        assertThat(future.join(),\n            is(\"road.kafka-offset.host.hostname.group.group_id.topic.test_topic.partition.0.offset 1 123\"));\n      });\n    }\n  }\n}",
                "name": "test",
                "modifiers": "@Test\n  public",
                "return_type": null,
                "parameters": [],
                "body": "{\n  kafka.createTopic(TOPIC);\n\n  try (KafkaConsumer<String, String> consumer = consumer()) {\n    consumer.commitSync(singletonMap(new TopicPartition(TOPIC, 0), new OffsetAndMetadata(1L)));\n  }\n\n  try (ServerSocket serverSocket = new ServerSocket(0)) {\n    CompletableFuture<String> future = CompletableFuture.supplyAsync(() -> {\n      try (Socket socket = serverSocket.accept();\n          BufferedReader reader = new BufferedReader(new InputStreamReader(socket.getInputStream()))) {\n        return reader.readLine();\n      } catch (Exception e) {\n        throw new RuntimeException(e);\n      }\n    });\n\n    try (ConfigurableApplicationContext context = runApp(serverSocket.getLocalPort())) {\n      Awaitility.await().atMost(5, SECONDS).pollInterval(100, MILLISECONDS).until(() -> {\n        assertThat(future.isDone(), is(true));\n        assertThat(future.join(),\n            is(\"road.kafka-offset.host.hostname.group.group_id.topic.test_topic.partition.0.offset 1 123\"));\n      });\n    }\n  }\n}",
                "start_point": {
                    "row": 83,
                    "column": 2
                },
                "end_point": {
                    "row": 109,
                    "column": 3
                }
            },
            {
                "definition": "private ConfigurableApplicationContext runApp(int port) {\n  String[] args = ImmutableMap\n      .<String, String> builder()\n      .put(\"kafka.bootstrapServers\", kafka.bootstrapServers())\n      .put(\"graphite.endpoint\", \"localhost:\" + port)\n      .put(\"metricRate\", \"1000\")\n      .build()\n      .entrySet()\n      .stream()\n      .map(e -> String.format(\"--%s=%s\", e.getKey(), e.getValue()))\n      .toArray(i -> new String[i]);\n  return new SpringApplicationBuilder(KafkaOffsetMetricsApp.class, TestConfig.class).bannerMode(OFF).run(args);\n}",
                "name": "runApp",
                "modifiers": "private",
                "return_type": "ConfigurableApplicationContext",
                "parameters": [
                    {
                        "type": "int",
                        "name": "port"
                    }
                ],
                "body": "{\n  String[] args = ImmutableMap\n      .<String, String> builder()\n      .put(\"kafka.bootstrapServers\", kafka.bootstrapServers())\n      .put(\"graphite.endpoint\", \"localhost:\" + port)\n      .put(\"metricRate\", \"1000\")\n      .build()\n      .entrySet()\n      .stream()\n      .map(e -> String.format(\"--%s=%s\", e.getKey(), e.getValue()))\n      .toArray(i -> new String[i]);\n  return new SpringApplicationBuilder(KafkaOffsetMetricsApp.class, TestConfig.class).bannerMode(OFF).run(args);\n}",
                "start_point": {
                    "row": 111,
                    "column": 2
                },
                "end_point": {
                    "row": 123,
                    "column": 3
                }
            },
            {
                "definition": "private KafkaConsumer<String, String> consumer() {\n  Map<String, Object> config = ImmutableMap\n      .<String, Object> builder()\n      .put(\"bootstrap.servers\", kafka.bootstrapServers())\n      .put(\"group.id\", \"group.id\")\n      .put(\"enable.auto.commit\", \"false\")\n      .put(\"auto.offset.reset\", \"earliest\")\n      .build();\n  Deserializer<String> deserializer = new StringDeserializer();\n  return new KafkaConsumer<>(config, deserializer, deserializer);\n}",
                "name": "consumer",
                "modifiers": "private",
                "return_type": null,
                "parameters": [],
                "body": "{\n  Map<String, Object> config = ImmutableMap\n      .<String, Object> builder()\n      .put(\"bootstrap.servers\", kafka.bootstrapServers())\n      .put(\"group.id\", \"group.id\")\n      .put(\"enable.auto.commit\", \"false\")\n      .put(\"auto.offset.reset\", \"earliest\")\n      .build();\n  Deserializer<String> deserializer = new StringDeserializer();\n  return new KafkaConsumer<>(config, deserializer, deserializer);\n}",
                "start_point": {
                    "row": 125,
                    "column": 2
                },
                "end_point": {
                    "row": 135,
                    "column": 3
                }
            }
        ]
    },
    {
        "rev_path": "monitoring/kafka-offset-metrics/src/test/java/com/hotels/road/kafka/offset/metrics/KafkaOffsetMetricsAppTest.java",
        "definition": "@Configuration\nstatic class TestConfig {\n  @Primary\n  @Bean\n  Clock clock() {\n    return new Clock() {\n      @Override\n      public long getTick() {\n        return 0L;\n      }\n\n      @Override\n      public long getTime() {\n        return 123000L;\n      }\n    };\n  }\n\n  @Primary\n  @Bean\n  Supplier<String> hostnameSupplier() {\n    return () -> \"hostname\";\n  }\n}",
        "package": "package com.hotels.road.kafka.offset.metrics;",
        "tree_path": "KafkaOffsetMetricsAppTest.TestConfig",
        "name": "TestConfig",
        "modifiers": "@Configuration\n  static",
        "superclass": null,
        "super_interfaces": null,
        "body": "{\n  @Primary\n  @Bean\n  Clock clock() {\n    return new Clock() {\n      @Override\n      public long getTick() {\n        return 0L;\n      }\n\n      @Override\n      public long getTime() {\n        return 123000L;\n      }\n    };\n  }\n\n  @Primary\n  @Bean\n  Supplier<String> hostnameSupplier() {\n    return () -> \"hostname\";\n  }\n}",
        "start_point": {
            "row": 58,
            "column": 2
        },
        "end_point": {
            "row": 81,
            "column": 3
        },
        "file_mode": "Modified",
        "map_path": null,
        "methods": [
            {
                "definition": "@Primary\n@Bean\nClock clock() {\n  return new Clock() {\n    @Override\n    public long getTick() {\n      return 0L;\n    }\n\n    @Override\n    public long getTime() {\n      return 123000L;\n    }\n  };\n}",
                "name": "clock",
                "modifiers": "@Primary\n    @Bean",
                "return_type": "Clock",
                "parameters": [],
                "body": "{\n  return new Clock() {\n    @Override\n    public long getTick() {\n      return 0L;\n    }\n\n    @Override\n    public long getTime() {\n      return 123000L;\n    }\n  };\n}",
                "start_point": {
                    "row": 60,
                    "column": 4
                },
                "end_point": {
                    "row": 74,
                    "column": 5
                }
            },
            {
                "definition": "@Primary\n@Bean\nSupplier<String> hostnameSupplier() {\n  return () -> \"hostname\";\n}",
                "name": "hostnameSupplier",
                "modifiers": "@Primary\n    @Bean",
                "return_type": null,
                "parameters": [],
                "body": "{\n  return () -> \"hostname\";\n}",
                "start_point": {
                    "row": 76,
                    "column": 4
                },
                "end_point": {
                    "row": 80,
                    "column": 5
                }
            }
        ]
    },
    {
        "rev_path": "monitoring/kafka-offset-metrics/src/test/java/com/hotels/road/kafka/offset/metrics/KafkaOffsetMetricsTest.java",
        "definition": "@RunWith(MockitoJUnitRunner.class)\npublic class KafkaOffsetMetricsTest {\n\n  @Mock\n  private AdminClient adminClient;\n  @Mock\n  private ScheduledReporter reporter;\n\n  private KafkaOffsetMetrics underTest;\n\n  @Before\n  public void before() {\n    underTest = new KafkaOffsetMetrics(adminClient, reporter);\n  }\n\n  @SuppressWarnings({ \"rawtypes\", \"unchecked\" })\n  @Test\n  public void happyPath() {\n    scala.collection.immutable.List<GroupOverview> listAllGroupsFlattened = asScalaSet(\n        singleton(GroupOverview.apply(\"groupId\", \"\"))).toList();\n    when(adminClient.listAllGroupsFlattened()).thenReturn(listAllGroupsFlattened);\n\n    scala.collection.immutable.Map<TopicPartition, Object> offsets = asScalaMap(\n        singletonMap(new TopicPartition(\"topicName\", 0), (Object) 1L));\n    when(adminClient.listGroupOffsets(\"groupId\")).thenReturn(offsets);\n\n    underTest.sendOffsets();\n\n    ArgumentCaptor<SortedMap<String, Gauge>> gaugesCaptor = ArgumentCaptor.forClass(SortedMap.class);\n\n    verify(reporter).report(gaugesCaptor.capture(), eq(emptySortedMap()), eq(emptySortedMap()), eq(emptySortedMap()),\n        eq(emptySortedMap()));\n\n    SortedMap<String, Gauge> gauges = gaugesCaptor.getValue();\n    assertThat(gauges.size(), is(1));\n    Gauge gauge = gauges.get(\"group.groupId.topic.topicName.partition.0.offset\");\n    assertThat(gauge.getValue(), is(1L));\n  }\n\n  private scala.collection.immutable.Map<TopicPartition, Object> asScalaMap(Map<TopicPartition, Object> map) {\n    return mapAsScalaMap(map).toMap(Predef.<Tuple2<TopicPartition, Object>> conforms());\n  }\n\n}",
        "package": "package com.hotels.road.kafka.offset.metrics;",
        "tree_path": "KafkaOffsetMetricsTest",
        "name": "KafkaOffsetMetricsTest",
        "modifiers": "@RunWith(MockitoJUnitRunner.class)\npublic",
        "superclass": null,
        "super_interfaces": null,
        "body": "{\n\n  @Mock\n  private AdminClient adminClient;\n  @Mock\n  private ScheduledReporter reporter;\n\n  private KafkaOffsetMetrics underTest;\n\n  @Before\n  public void before() {\n    underTest = new KafkaOffsetMetrics(adminClient, reporter);\n  }\n\n  @SuppressWarnings({ \"rawtypes\", \"unchecked\" })\n  @Test\n  public void happyPath() {\n    scala.collection.immutable.List<GroupOverview> listAllGroupsFlattened = asScalaSet(\n        singleton(GroupOverview.apply(\"groupId\", \"\"))).toList();\n    when(adminClient.listAllGroupsFlattened()).thenReturn(listAllGroupsFlattened);\n\n    scala.collection.immutable.Map<TopicPartition, Object> offsets = asScalaMap(\n        singletonMap(new TopicPartition(\"topicName\", 0), (Object) 1L));\n    when(adminClient.listGroupOffsets(\"groupId\")).thenReturn(offsets);\n\n    underTest.sendOffsets();\n\n    ArgumentCaptor<SortedMap<String, Gauge>> gaugesCaptor = ArgumentCaptor.forClass(SortedMap.class);\n\n    verify(reporter).report(gaugesCaptor.capture(), eq(emptySortedMap()), eq(emptySortedMap()), eq(emptySortedMap()),\n        eq(emptySortedMap()));\n\n    SortedMap<String, Gauge> gauges = gaugesCaptor.getValue();\n    assertThat(gauges.size(), is(1));\n    Gauge gauge = gauges.get(\"group.groupId.topic.topicName.partition.0.offset\");\n    assertThat(gauge.getValue(), is(1L));\n  }\n\n  private scala.collection.immutable.Map<TopicPartition, Object> asScalaMap(Map<TopicPartition, Object> map) {\n    return mapAsScalaMap(map).toMap(Predef.<Tuple2<TopicPartition, Object>> conforms());\n  }\n\n}",
        "start_point": {
            "row": 49,
            "column": 0
        },
        "end_point": {
            "row": 92,
            "column": 1
        },
        "file_mode": "Modified",
        "map_path": null,
        "methods": [
            {
                "definition": "@Before\npublic void before() {\n  underTest = new KafkaOffsetMetrics(adminClient, reporter);\n}",
                "name": "before",
                "modifiers": "@Before\n  public",
                "return_type": null,
                "parameters": [],
                "body": "{\n  underTest = new KafkaOffsetMetrics(adminClient, reporter);\n}",
                "start_point": {
                    "row": 59,
                    "column": 2
                },
                "end_point": {
                    "row": 62,
                    "column": 3
                }
            },
            {
                "definition": "@SuppressWarnings({ \"rawtypes\", \"unchecked\" })\n@Test\npublic void happyPath() {\n  scala.collection.immutable.List<GroupOverview> listAllGroupsFlattened = asScalaSet(\n      singleton(GroupOverview.apply(\"groupId\", \"\"))).toList();\n  when(adminClient.listAllGroupsFlattened()).thenReturn(listAllGroupsFlattened);\n\n  scala.collection.immutable.Map<TopicPartition, Object> offsets = asScalaMap(\n      singletonMap(new TopicPartition(\"topicName\", 0), (Object) 1L));\n  when(adminClient.listGroupOffsets(\"groupId\")).thenReturn(offsets);\n\n  underTest.sendOffsets();\n\n  ArgumentCaptor<SortedMap<String, Gauge>> gaugesCaptor = ArgumentCaptor.forClass(SortedMap.class);\n\n  verify(reporter).report(gaugesCaptor.capture(), eq(emptySortedMap()), eq(emptySortedMap()), eq(emptySortedMap()),\n      eq(emptySortedMap()));\n\n  SortedMap<String, Gauge> gauges = gaugesCaptor.getValue();\n  assertThat(gauges.size(), is(1));\n  Gauge gauge = gauges.get(\"group.groupId.topic.topicName.partition.0.offset\");\n  assertThat(gauge.getValue(), is(1L));\n}",
                "name": "happyPath",
                "modifiers": "@SuppressWarnings({ \"rawtypes\", \"unchecked\" })\n  @Test\n  public",
                "return_type": null,
                "parameters": [],
                "body": "{\n  scala.collection.immutable.List<GroupOverview> listAllGroupsFlattened = asScalaSet(\n      singleton(GroupOverview.apply(\"groupId\", \"\"))).toList();\n  when(adminClient.listAllGroupsFlattened()).thenReturn(listAllGroupsFlattened);\n\n  scala.collection.immutable.Map<TopicPartition, Object> offsets = asScalaMap(\n      singletonMap(new TopicPartition(\"topicName\", 0), (Object) 1L));\n  when(adminClient.listGroupOffsets(\"groupId\")).thenReturn(offsets);\n\n  underTest.sendOffsets();\n\n  ArgumentCaptor<SortedMap<String, Gauge>> gaugesCaptor = ArgumentCaptor.forClass(SortedMap.class);\n\n  verify(reporter).report(gaugesCaptor.capture(), eq(emptySortedMap()), eq(emptySortedMap()), eq(emptySortedMap()),\n      eq(emptySortedMap()));\n\n  SortedMap<String, Gauge> gauges = gaugesCaptor.getValue();\n  assertThat(gauges.size(), is(1));\n  Gauge gauge = gauges.get(\"group.groupId.topic.topicName.partition.0.offset\");\n  assertThat(gauge.getValue(), is(1L));\n}",
                "start_point": {
                    "row": 64,
                    "column": 2
                },
                "end_point": {
                    "row": 86,
                    "column": 3
                }
            },
            {
                "definition": "private scala.collection.immutable.Map<TopicPartition, Object> asScalaMap(Map<TopicPartition, Object> map) {\n  return mapAsScalaMap(map).toMap(Predef.<Tuple2<TopicPartition, Object>> conforms());\n}",
                "name": "asScalaMap",
                "modifiers": "private",
                "return_type": null,
                "parameters": [
                    {
                        "type": "Map<TopicPartition, Object>",
                        "name": "map"
                    }
                ],
                "body": "{\n  return mapAsScalaMap(map).toMap(Predef.<Tuple2<TopicPartition, Object>> conforms());\n}",
                "start_point": {
                    "row": 88,
                    "column": 2
                },
                "end_point": {
                    "row": 90,
                    "column": 3
                }
            }
        ]
    },
    {
        "rev_path": "agent/tollbooth/road-tollbooth-app/src/main/java/com/hotels/road/tollbooth/app/metrics/MetricsConfiguration.java",
        "definition": "@Configuration\n@EnableMetrics(proxyTargetClass = true)\n@Slf4j\npublic class MetricsConfiguration extends MetricsConfigurerAdapter {\n  private final InetSocketAddress graphiteEndpoint;\n\n  @Autowired\n  public MetricsConfiguration(@Value(\"${graphite.endpoint:disabled}\") String graphiteEndpoint) {\n    if (\"disabled\".equalsIgnoreCase(graphiteEndpoint)) {\n      log.info(\"Graphite metrics reporting is disabled\");\n      this.graphiteEndpoint = null;\n    } else {\n      log.info(\"Graphite reporting is configured for {}\", graphiteEndpoint);\n      HostAndPort hostAndPort = HostAndPort.fromString(graphiteEndpoint);\n      this.graphiteEndpoint = new InetSocketAddress(hostAndPort.getHost(), hostAndPort.getPort());\n    }\n  }\n\n  @Override\n  public void configureReporters(MetricRegistry registry) {\n    try {\n      log.info(\"Adding JVM metric sets to metrics registry\");\n      registry.register(jvmName(\"gc\"), new GarbageCollectorMetricSet());\n      registry.register(jvmName(\"buffers\"), new BufferPoolMetricSet(ManagementFactory.getPlatformMBeanServer()));\n      registry.register(jvmName(\"memory\"), new MemoryUsageGaugeSet());\n      registry.register(jvmName(\"threads\"), new ThreadStatesGaugeSet());\n    } catch (IllegalArgumentException e) {\n      log.warn(\"Exception adding JVM metric sets\", e);\n    }\n\n    if (graphiteEndpoint != null) {\n      log.info(\"Starting Graphite reporter\");\n      registerReporter(GraphiteReporter\n          .forRegistry(registry)\n          .prefixedWith(MetricRegistry.name(\"road\", \"tollbooth\", \"host\", getHostname()))\n          .convertRatesTo(SECONDS)\n          .convertDurationsTo(MILLISECONDS)\n          .build(new Graphite(graphiteEndpoint))).start(10, SECONDS);\n    }\n  }\n\n  @Bean\n  public MetricsServletsContextListener metricsServletsContextListener() {\n    return new MetricsServletsContextListener();\n  }\n\n  @Bean\n  public ServletRegistrationBean<?> adminServletRegistration() {\n    return new ServletRegistrationBean<>(new com.codahale.metrics.servlets.AdminServlet(), \"/admin/*\");\n  }\n\n  @Bean\n  public ServletRegistrationBean<?> prometheusServletRegistration(MetricRegistry registry) {\n    CollectorRegistry.defaultRegistry.register(new DropwizardExports(registry));\n    return new ServletRegistrationBean<>(new io.prometheus.client.exporter.MetricsServlet(), \"/metrics\");\n  }\n\n  private String jvmName(String name) {\n    return MetricRegistry.name(\"jvm\", name);\n  }\n\n  private String getHostname() {\n    try {\n      return InetAddress.getLocalHost().getHostName();\n    } catch (UnknownHostException e) {\n      throw new RuntimeException(e);\n    }\n  }\n\n}",
        "package": "package com.hotels.road.tollbooth.app.metrics;",
        "tree_path": "MetricsConfiguration",
        "name": "MetricsConfiguration",
        "modifiers": "@Configuration\n@EnableMetrics(proxyTargetClass = true)\n@Slf4j\npublic",
        "superclass": "extends MetricsConfigurerAdapter",
        "super_interfaces": null,
        "body": "{\n  private final InetSocketAddress graphiteEndpoint;\n\n  @Autowired\n  public MetricsConfiguration(@Value(\"${graphite.endpoint:disabled}\") String graphiteEndpoint) {\n    if (\"disabled\".equalsIgnoreCase(graphiteEndpoint)) {\n      log.info(\"Graphite metrics reporting is disabled\");\n      this.graphiteEndpoint = null;\n    } else {\n      log.info(\"Graphite reporting is configured for {}\", graphiteEndpoint);\n      HostAndPort hostAndPort = HostAndPort.fromString(graphiteEndpoint);\n      this.graphiteEndpoint = new InetSocketAddress(hostAndPort.getHost(), hostAndPort.getPort());\n    }\n  }\n\n  @Override\n  public void configureReporters(MetricRegistry registry) {\n    try {\n      log.info(\"Adding JVM metric sets to metrics registry\");\n      registry.register(jvmName(\"gc\"), new GarbageCollectorMetricSet());\n      registry.register(jvmName(\"buffers\"), new BufferPoolMetricSet(ManagementFactory.getPlatformMBeanServer()));\n      registry.register(jvmName(\"memory\"), new MemoryUsageGaugeSet());\n      registry.register(jvmName(\"threads\"), new ThreadStatesGaugeSet());\n    } catch (IllegalArgumentException e) {\n      log.warn(\"Exception adding JVM metric sets\", e);\n    }\n\n    if (graphiteEndpoint != null) {\n      log.info(\"Starting Graphite reporter\");\n      registerReporter(GraphiteReporter\n          .forRegistry(registry)\n          .prefixedWith(MetricRegistry.name(\"road\", \"tollbooth\", \"host\", getHostname()))\n          .convertRatesTo(SECONDS)\n          .convertDurationsTo(MILLISECONDS)\n          .build(new Graphite(graphiteEndpoint))).start(10, SECONDS);\n    }\n  }\n\n  @Bean\n  public MetricsServletsContextListener metricsServletsContextListener() {\n    return new MetricsServletsContextListener();\n  }\n\n  @Bean\n  public ServletRegistrationBean<?> adminServletRegistration() {\n    return new ServletRegistrationBean<>(new com.codahale.metrics.servlets.AdminServlet(), \"/admin/*\");\n  }\n\n  @Bean\n  public ServletRegistrationBean<?> prometheusServletRegistration(MetricRegistry registry) {\n    CollectorRegistry.defaultRegistry.register(new DropwizardExports(registry));\n    return new ServletRegistrationBean<>(new io.prometheus.client.exporter.MetricsServlet(), \"/metrics\");\n  }\n\n  private String jvmName(String name) {\n    return MetricRegistry.name(\"jvm\", name);\n  }\n\n  private String getHostname() {\n    try {\n      return InetAddress.getLocalHost().getHostName();\n    } catch (UnknownHostException e) {\n      throw new RuntimeException(e);\n    }\n  }\n\n}",
        "start_point": {
            "row": 47,
            "column": 0
        },
        "end_point": {
            "row": 116,
            "column": 1
        },
        "file_mode": "Deleted",
        "map_path": null,
        "methods": [
            {
                "definition": "@Autowired\npublic MetricsConfiguration(@Value(\"${graphite.endpoint:disabled}\") String graphiteEndpoint) {\n  if (\"disabled\".equalsIgnoreCase(graphiteEndpoint)) {\n    log.info(\"Graphite metrics reporting is disabled\");\n    this.graphiteEndpoint = null;\n  } else {\n    log.info(\"Graphite reporting is configured for {}\", graphiteEndpoint);\n    HostAndPort hostAndPort = HostAndPort.fromString(graphiteEndpoint);\n    this.graphiteEndpoint = new InetSocketAddress(hostAndPort.getHost(), hostAndPort.getPort());\n  }\n}",
                "name": "MetricsConfiguration",
                "modifiers": "@Autowired\n  public",
                "parameters": [
                    {
                        "type": "@Value(\"${graphite.endpoint:disabled}\")",
                        "name": "String"
                    }
                ],
                "body": "{\n  if (\"disabled\".equalsIgnoreCase(graphiteEndpoint)) {\n    log.info(\"Graphite metrics reporting is disabled\");\n    this.graphiteEndpoint = null;\n  } else {\n    log.info(\"Graphite reporting is configured for {}\", graphiteEndpoint);\n    HostAndPort hostAndPort = HostAndPort.fromString(graphiteEndpoint);\n    this.graphiteEndpoint = new InetSocketAddress(hostAndPort.getHost(), hostAndPort.getPort());\n  }\n}",
                "constructor": true,
                "start_point": {
                    "row": 53,
                    "column": 2
                },
                "end_point": {
                    "row": 63,
                    "column": 3
                }
            },
            {
                "definition": "@Override\npublic void configureReporters(MetricRegistry registry) {\n  try {\n    log.info(\"Adding JVM metric sets to metrics registry\");\n    registry.register(jvmName(\"gc\"), new GarbageCollectorMetricSet());\n    registry.register(jvmName(\"buffers\"), new BufferPoolMetricSet(ManagementFactory.getPlatformMBeanServer()));\n    registry.register(jvmName(\"memory\"), new MemoryUsageGaugeSet());\n    registry.register(jvmName(\"threads\"), new ThreadStatesGaugeSet());\n  } catch (IllegalArgumentException e) {\n    log.warn(\"Exception adding JVM metric sets\", e);\n  }\n\n  if (graphiteEndpoint != null) {\n    log.info(\"Starting Graphite reporter\");\n    registerReporter(GraphiteReporter\n        .forRegistry(registry)\n        .prefixedWith(MetricRegistry.name(\"road\", \"tollbooth\", \"host\", getHostname()))\n        .convertRatesTo(SECONDS)\n        .convertDurationsTo(MILLISECONDS)\n        .build(new Graphite(graphiteEndpoint))).start(10, SECONDS);\n  }\n}",
                "name": "configureReporters",
                "modifiers": "@Override\n  public",
                "return_type": null,
                "parameters": [
                    {
                        "type": "MetricRegistry",
                        "name": "registry"
                    }
                ],
                "body": "{\n  try {\n    log.info(\"Adding JVM metric sets to metrics registry\");\n    registry.register(jvmName(\"gc\"), new GarbageCollectorMetricSet());\n    registry.register(jvmName(\"buffers\"), new BufferPoolMetricSet(ManagementFactory.getPlatformMBeanServer()));\n    registry.register(jvmName(\"memory\"), new MemoryUsageGaugeSet());\n    registry.register(jvmName(\"threads\"), new ThreadStatesGaugeSet());\n  } catch (IllegalArgumentException e) {\n    log.warn(\"Exception adding JVM metric sets\", e);\n  }\n\n  if (graphiteEndpoint != null) {\n    log.info(\"Starting Graphite reporter\");\n    registerReporter(GraphiteReporter\n        .forRegistry(registry)\n        .prefixedWith(MetricRegistry.name(\"road\", \"tollbooth\", \"host\", getHostname()))\n        .convertRatesTo(SECONDS)\n        .convertDurationsTo(MILLISECONDS)\n        .build(new Graphite(graphiteEndpoint))).start(10, SECONDS);\n  }\n}",
                "start_point": {
                    "row": 65,
                    "column": 2
                },
                "end_point": {
                    "row": 86,
                    "column": 3
                }
            },
            {
                "definition": "@Bean\npublic MetricsServletsContextListener metricsServletsContextListener() {\n  return new MetricsServletsContextListener();\n}",
                "name": "metricsServletsContextListener",
                "modifiers": "@Bean\n  public",
                "return_type": "MetricsServletsContextListener",
                "parameters": [],
                "body": "{\n  return new MetricsServletsContextListener();\n}",
                "start_point": {
                    "row": 88,
                    "column": 2
                },
                "end_point": {
                    "row": 91,
                    "column": 3
                }
            },
            {
                "definition": "@Bean\npublic ServletRegistrationBean<?> adminServletRegistration() {\n  return new ServletRegistrationBean<>(new com.codahale.metrics.servlets.AdminServlet(), \"/admin/*\");\n}",
                "name": "adminServletRegistration",
                "modifiers": "@Bean\n  public",
                "return_type": null,
                "parameters": [],
                "body": "{\n  return new ServletRegistrationBean<>(new com.codahale.metrics.servlets.AdminServlet(), \"/admin/*\");\n}",
                "start_point": {
                    "row": 93,
                    "column": 2
                },
                "end_point": {
                    "row": 96,
                    "column": 3
                }
            },
            {
                "definition": "@Bean\npublic ServletRegistrationBean<?> prometheusServletRegistration(MetricRegistry registry) {\n  CollectorRegistry.defaultRegistry.register(new DropwizardExports(registry));\n  return new ServletRegistrationBean<>(new io.prometheus.client.exporter.MetricsServlet(), \"/metrics\");\n}",
                "name": "prometheusServletRegistration",
                "modifiers": "@Bean\n  public",
                "return_type": null,
                "parameters": [
                    {
                        "type": "MetricRegistry",
                        "name": "registry"
                    }
                ],
                "body": "{\n  CollectorRegistry.defaultRegistry.register(new DropwizardExports(registry));\n  return new ServletRegistrationBean<>(new io.prometheus.client.exporter.MetricsServlet(), \"/metrics\");\n}",
                "start_point": {
                    "row": 98,
                    "column": 2
                },
                "end_point": {
                    "row": 102,
                    "column": 3
                }
            },
            {
                "definition": "private String jvmName(String name) {\n  return MetricRegistry.name(\"jvm\", name);\n}",
                "name": "jvmName",
                "modifiers": "private",
                "return_type": "String",
                "parameters": [
                    {
                        "type": "String",
                        "name": "name"
                    }
                ],
                "body": "{\n  return MetricRegistry.name(\"jvm\", name);\n}",
                "start_point": {
                    "row": 104,
                    "column": 2
                },
                "end_point": {
                    "row": 106,
                    "column": 3
                }
            },
            {
                "definition": "private String getHostname() {\n  try {\n    return InetAddress.getLocalHost().getHostName();\n  } catch (UnknownHostException e) {\n    throw new RuntimeException(e);\n  }\n}",
                "name": "getHostname",
                "modifiers": "private",
                "return_type": "String",
                "parameters": [],
                "body": "{\n  try {\n    return InetAddress.getLocalHost().getHostName();\n  } catch (UnknownHostException e) {\n    throw new RuntimeException(e);\n  }\n}",
                "start_point": {
                    "row": 108,
                    "column": 2
                },
                "end_point": {
                    "row": 114,
                    "column": 3
                }
            }
        ]
    }
]