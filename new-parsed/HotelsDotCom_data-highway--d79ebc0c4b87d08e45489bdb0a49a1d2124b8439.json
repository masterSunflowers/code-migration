[
    {
        "rev_path": "agent/tollbooth/road-tollbooth-app/src/test/java/com/hotels/road/tollbooth/app/TollboothAppIntegrationTest.java",
        "definition": "public class TollboothAppIntegrationTest {\n  private static final String ROAD_TOPIC = \"road\";\n  private static final String PATCH_TOPIC = \"patch\";\n\n  private static final int NUM_BROKERS = 1;\n\n  private static final ObjectMapper mapper = new ObjectMapper();\n\n  @ClassRule\n  public static EmbeddedKafkaCluster kafka = new EmbeddedKafkaCluster(NUM_BROKERS);\n  private static ConfigurableApplicationContext context;\n\n  private static KafkaProducer<String, String> producer;\n\n  private static Map<String, JsonNode> store;\n\n  private static int port;\n\n  @BeforeClass\n  public static void beforeClass() throws Exception {\n    try (ServerSocket socket = new ServerSocket(0)) {\n      port = socket.getLocalPort();\n    }\n\n    context = new SpringApplicationBuilder(TollboothApp.class)\n        .bannerMode(OFF)\n        .properties(ImmutableMap\n            .<String, Object> builder()\n            .put(\"server.port\", port)\n            .put(\"kafka.bootstrapServers\", kafka.bootstrapServers())\n            .put(\"kafka.zookeeper\", kafka.zKConnectString())\n            .put(\"kafka.store.topic\", ROAD_TOPIC)\n            .put(\"kafka.store.replicas\", \"1\")\n            .put(\"kafka.patch.topic\", PATCH_TOPIC)\n            .put(\"kafka.patch.replicas\", \"1\")\n            .put(\"kafka.patch.groupId\", \"patches\")\n            .build())\n        .build()\n        .run();\n\n    Properties properties = new Properties();\n    properties.setProperty(\"bootstrap.servers\", kafka.bootstrapServers());\n    producer = new KafkaProducer<>(properties, new StringSerializer(), new StringSerializer());\n\n    store = new KafkaStore<>(kafka.bootstrapServers(), new JsonNodeSerializer(mapper), ROAD_TOPIC);\n  }\n\n  @Test(timeout = 20000)\n  public void create_document_from_patch() throws Exception {\n    producer.send(new ProducerRecord<>(PATCH_TOPIC,\n        \"{\\\"documentId\\\":\\\"road0\\\",\\\"operations\\\":[{\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"\\\",\\\"value\\\":{\\\"name\\\":\\\"hello\\\"}}]}\"));\n\n    while (true) {\n      JsonNode result = store.get(\"road0\");\n      if (result != null) {\n        assertThat(result.path(\"name\").textValue(), is(\"hello\"));\n        return;\n      }\n      Thread.sleep(100);\n    }\n  }\n\n  @Test(timeout = 20000)\n  public void bad_patchs_are_skipped() throws Exception {\n    store.put(\"road1\", mapper.readTree(\"{\\\"name\\\":\\\"hello\\\"}\"));\n\n    producer.send(new ProducerRecord<>(PATCH_TOPIC, \"not json\"));\n    producer.send(new ProducerRecord<>(PATCH_TOPIC,\n        \"{\\\"documentId\\\":\\\"road1\\\",\\\"operations\\\":[{\\\"op\\\":\\\"replace\\\",\\\"path\\\":\\\"/name\\\",\\\"value\\\":\\\"hi there\\\"}]}\"));\n\n    while (!\"hi there\".equals(store.get(\"road1\").path(\"name\").asText())) {\n      Thread.sleep(100);\n    }\n  }\n\n  @Test(timeout = 20000)\n  public void remove_document() throws Exception {\n    store.put(\"road2\", mapper.readTree(\"{\\\"name\\\":\\\"goodbye\\\"}\"));\n\n    producer.send(new ProducerRecord<>(PATCH_TOPIC,\n        \"{\\\"documentId\\\":\\\"road2\\\",\\\"operations\\\":[{\\\"op\\\":\\\"remove\\\",\\\"path\\\":\\\"\\\"}]}\"));\n\n    while (store.containsKey(\"road2\")) {\n      Thread.sleep(100);\n    }\n  }\n\n  @Test(timeout = 20000)\n  public void testMetrics() throws Exception {\n    RestTemplate restTemplate = new RestTemplate();\n    String fooResourceUrl = \"http://localhost:\" + port + \"/actuator/prometheus\";\n\n    Awaitility.await().atMost(5, SECONDS).pollInterval(100, MILLISECONDS).until(() -> {\n      ResponseEntity<String> response = restTemplate.getForEntity(fooResourceUrl, String.class);\n      assertThat(response.getStatusCode(), equalTo(HttpStatus.OK));\n      List<String> lines = Arrays.asList(response.getBody().split(\"\\n\")).stream().filter(l -> !l.startsWith(\"#\")).collect(toList());\n      assertThat(lines, not(emptyList()));\n    });\n  }\n\n  @AfterClass\n  public static void after() {\n    if (context != null) {\n      context.close();\n    }\n  }\n}",
        "package": "package com.hotels.road.tollbooth.app;",
        "tree_path": "TollboothAppIntegrationTest",
        "name": "TollboothAppIntegrationTest",
        "modifiers": "public",
        "superclass": null,
        "super_interfaces": null,
        "body": "{\n  private static final String ROAD_TOPIC = \"road\";\n  private static final String PATCH_TOPIC = \"patch\";\n\n  private static final int NUM_BROKERS = 1;\n\n  private static final ObjectMapper mapper = new ObjectMapper();\n\n  @ClassRule\n  public static EmbeddedKafkaCluster kafka = new EmbeddedKafkaCluster(NUM_BROKERS);\n  private static ConfigurableApplicationContext context;\n\n  private static KafkaProducer<String, String> producer;\n\n  private static Map<String, JsonNode> store;\n\n  private static int port;\n\n  @BeforeClass\n  public static void beforeClass() throws Exception {\n    try (ServerSocket socket = new ServerSocket(0)) {\n      port = socket.getLocalPort();\n    }\n\n    context = new SpringApplicationBuilder(TollboothApp.class)\n        .bannerMode(OFF)\n        .properties(ImmutableMap\n            .<String, Object> builder()\n            .put(\"server.port\", port)\n            .put(\"kafka.bootstrapServers\", kafka.bootstrapServers())\n            .put(\"kafka.zookeeper\", kafka.zKConnectString())\n            .put(\"kafka.store.topic\", ROAD_TOPIC)\n            .put(\"kafka.store.replicas\", \"1\")\n            .put(\"kafka.patch.topic\", PATCH_TOPIC)\n            .put(\"kafka.patch.replicas\", \"1\")\n            .put(\"kafka.patch.groupId\", \"patches\")\n            .build())\n        .build()\n        .run();\n\n    Properties properties = new Properties();\n    properties.setProperty(\"bootstrap.servers\", kafka.bootstrapServers());\n    producer = new KafkaProducer<>(properties, new StringSerializer(), new StringSerializer());\n\n    store = new KafkaStore<>(kafka.bootstrapServers(), new JsonNodeSerializer(mapper), ROAD_TOPIC);\n  }\n\n  @Test(timeout = 20000)\n  public void create_document_from_patch() throws Exception {\n    producer.send(new ProducerRecord<>(PATCH_TOPIC,\n        \"{\\\"documentId\\\":\\\"road0\\\",\\\"operations\\\":[{\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"\\\",\\\"value\\\":{\\\"name\\\":\\\"hello\\\"}}]}\"));\n\n    while (true) {\n      JsonNode result = store.get(\"road0\");\n      if (result != null) {\n        assertThat(result.path(\"name\").textValue(), is(\"hello\"));\n        return;\n      }\n      Thread.sleep(100);\n    }\n  }\n\n  @Test(timeout = 20000)\n  public void bad_patchs_are_skipped() throws Exception {\n    store.put(\"road1\", mapper.readTree(\"{\\\"name\\\":\\\"hello\\\"}\"));\n\n    producer.send(new ProducerRecord<>(PATCH_TOPIC, \"not json\"));\n    producer.send(new ProducerRecord<>(PATCH_TOPIC,\n        \"{\\\"documentId\\\":\\\"road1\\\",\\\"operations\\\":[{\\\"op\\\":\\\"replace\\\",\\\"path\\\":\\\"/name\\\",\\\"value\\\":\\\"hi there\\\"}]}\"));\n\n    while (!\"hi there\".equals(store.get(\"road1\").path(\"name\").asText())) {\n      Thread.sleep(100);\n    }\n  }\n\n  @Test(timeout = 20000)\n  public void remove_document() throws Exception {\n    store.put(\"road2\", mapper.readTree(\"{\\\"name\\\":\\\"goodbye\\\"}\"));\n\n    producer.send(new ProducerRecord<>(PATCH_TOPIC,\n        \"{\\\"documentId\\\":\\\"road2\\\",\\\"operations\\\":[{\\\"op\\\":\\\"remove\\\",\\\"path\\\":\\\"\\\"}]}\"));\n\n    while (store.containsKey(\"road2\")) {\n      Thread.sleep(100);\n    }\n  }\n\n  @Test(timeout = 20000)\n  public void testMetrics() throws Exception {\n    RestTemplate restTemplate = new RestTemplate();\n    String fooResourceUrl = \"http://localhost:\" + port + \"/actuator/prometheus\";\n\n    Awaitility.await().atMost(5, SECONDS).pollInterval(100, MILLISECONDS).until(() -> {\n      ResponseEntity<String> response = restTemplate.getForEntity(fooResourceUrl, String.class);\n      assertThat(response.getStatusCode(), equalTo(HttpStatus.OK));\n      List<String> lines = Arrays.asList(response.getBody().split(\"\\n\")).stream().filter(l -> !l.startsWith(\"#\")).collect(toList());\n      assertThat(lines, not(emptyList()));\n    });\n  }\n\n  @AfterClass\n  public static void after() {\n    if (context != null) {\n      context.close();\n    }\n  }\n}",
        "start_point": {
            "row": 54,
            "column": 0
        },
        "end_point": {
            "row": 160,
            "column": 1
        },
        "file_mode": "Modified",
        "map_path": null,
        "methods": [
            {
                "definition": "@BeforeClass\npublic static void beforeClass() throws Exception {\n  try (ServerSocket socket = new ServerSocket(0)) {\n    port = socket.getLocalPort();\n  }\n\n  context = new SpringApplicationBuilder(TollboothApp.class)\n      .bannerMode(OFF)\n      .properties(ImmutableMap\n          .<String, Object> builder()\n          .put(\"server.port\", port)\n          .put(\"kafka.bootstrapServers\", kafka.bootstrapServers())\n          .put(\"kafka.zookeeper\", kafka.zKConnectString())\n          .put(\"kafka.store.topic\", ROAD_TOPIC)\n          .put(\"kafka.store.replicas\", \"1\")\n          .put(\"kafka.patch.topic\", PATCH_TOPIC)\n          .put(\"kafka.patch.replicas\", \"1\")\n          .put(\"kafka.patch.groupId\", \"patches\")\n          .build())\n      .build()\n      .run();\n\n  Properties properties = new Properties();\n  properties.setProperty(\"bootstrap.servers\", kafka.bootstrapServers());\n  producer = new KafkaProducer<>(properties, new StringSerializer(), new StringSerializer());\n\n  store = new KafkaStore<>(kafka.bootstrapServers(), new JsonNodeSerializer(mapper), ROAD_TOPIC);\n}",
                "name": "beforeClass",
                "modifiers": "@BeforeClass\n  public static",
                "return_type": null,
                "parameters": [],
                "body": "{\n  try (ServerSocket socket = new ServerSocket(0)) {\n    port = socket.getLocalPort();\n  }\n\n  context = new SpringApplicationBuilder(TollboothApp.class)\n      .bannerMode(OFF)\n      .properties(ImmutableMap\n          .<String, Object> builder()\n          .put(\"server.port\", port)\n          .put(\"kafka.bootstrapServers\", kafka.bootstrapServers())\n          .put(\"kafka.zookeeper\", kafka.zKConnectString())\n          .put(\"kafka.store.topic\", ROAD_TOPIC)\n          .put(\"kafka.store.replicas\", \"1\")\n          .put(\"kafka.patch.topic\", PATCH_TOPIC)\n          .put(\"kafka.patch.replicas\", \"1\")\n          .put(\"kafka.patch.groupId\", \"patches\")\n          .build())\n      .build()\n      .run();\n\n  Properties properties = new Properties();\n  properties.setProperty(\"bootstrap.servers\", kafka.bootstrapServers());\n  producer = new KafkaProducer<>(properties, new StringSerializer(), new StringSerializer());\n\n  store = new KafkaStore<>(kafka.bootstrapServers(), new JsonNodeSerializer(mapper), ROAD_TOPIC);\n}",
                "start_point": {
                    "row": 72,
                    "column": 2
                },
                "end_point": {
                    "row": 99,
                    "column": 3
                }
            },
            {
                "definition": "@Test(timeout = 20000)\npublic void create_document_from_patch() throws Exception {\n  producer.send(new ProducerRecord<>(PATCH_TOPIC,\n      \"{\\\"documentId\\\":\\\"road0\\\",\\\"operations\\\":[{\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"\\\",\\\"value\\\":{\\\"name\\\":\\\"hello\\\"}}]}\"));\n\n  while (true) {\n    JsonNode result = store.get(\"road0\");\n    if (result != null) {\n      assertThat(result.path(\"name\").textValue(), is(\"hello\"));\n      return;\n    }\n    Thread.sleep(100);\n  }\n}",
                "name": "create_document_from_patch",
                "modifiers": "@Test(timeout = 20000)\n  public",
                "return_type": null,
                "parameters": [],
                "body": "{\n  producer.send(new ProducerRecord<>(PATCH_TOPIC,\n      \"{\\\"documentId\\\":\\\"road0\\\",\\\"operations\\\":[{\\\"op\\\":\\\"add\\\",\\\"path\\\":\\\"\\\",\\\"value\\\":{\\\"name\\\":\\\"hello\\\"}}]}\"));\n\n  while (true) {\n    JsonNode result = store.get(\"road0\");\n    if (result != null) {\n      assertThat(result.path(\"name\").textValue(), is(\"hello\"));\n      return;\n    }\n    Thread.sleep(100);\n  }\n}",
                "start_point": {
                    "row": 101,
                    "column": 2
                },
                "end_point": {
                    "row": 114,
                    "column": 3
                }
            },
            {
                "definition": "@Test(timeout = 20000)\npublic void bad_patchs_are_skipped() throws Exception {\n  store.put(\"road1\", mapper.readTree(\"{\\\"name\\\":\\\"hello\\\"}\"));\n\n  producer.send(new ProducerRecord<>(PATCH_TOPIC, \"not json\"));\n  producer.send(new ProducerRecord<>(PATCH_TOPIC,\n      \"{\\\"documentId\\\":\\\"road1\\\",\\\"operations\\\":[{\\\"op\\\":\\\"replace\\\",\\\"path\\\":\\\"/name\\\",\\\"value\\\":\\\"hi there\\\"}]}\"));\n\n  while (!\"hi there\".equals(store.get(\"road1\").path(\"name\").asText())) {\n    Thread.sleep(100);\n  }\n}",
                "name": "bad_patchs_are_skipped",
                "modifiers": "@Test(timeout = 20000)\n  public",
                "return_type": null,
                "parameters": [],
                "body": "{\n  store.put(\"road1\", mapper.readTree(\"{\\\"name\\\":\\\"hello\\\"}\"));\n\n  producer.send(new ProducerRecord<>(PATCH_TOPIC, \"not json\"));\n  producer.send(new ProducerRecord<>(PATCH_TOPIC,\n      \"{\\\"documentId\\\":\\\"road1\\\",\\\"operations\\\":[{\\\"op\\\":\\\"replace\\\",\\\"path\\\":\\\"/name\\\",\\\"value\\\":\\\"hi there\\\"}]}\"));\n\n  while (!\"hi there\".equals(store.get(\"road1\").path(\"name\").asText())) {\n    Thread.sleep(100);\n  }\n}",
                "start_point": {
                    "row": 116,
                    "column": 2
                },
                "end_point": {
                    "row": 127,
                    "column": 3
                }
            },
            {
                "definition": "@Test(timeout = 20000)\npublic void remove_document() throws Exception {\n  store.put(\"road2\", mapper.readTree(\"{\\\"name\\\":\\\"goodbye\\\"}\"));\n\n  producer.send(new ProducerRecord<>(PATCH_TOPIC,\n      \"{\\\"documentId\\\":\\\"road2\\\",\\\"operations\\\":[{\\\"op\\\":\\\"remove\\\",\\\"path\\\":\\\"\\\"}]}\"));\n\n  while (store.containsKey(\"road2\")) {\n    Thread.sleep(100);\n  }\n}",
                "name": "remove_document",
                "modifiers": "@Test(timeout = 20000)\n  public",
                "return_type": null,
                "parameters": [],
                "body": "{\n  store.put(\"road2\", mapper.readTree(\"{\\\"name\\\":\\\"goodbye\\\"}\"));\n\n  producer.send(new ProducerRecord<>(PATCH_TOPIC,\n      \"{\\\"documentId\\\":\\\"road2\\\",\\\"operations\\\":[{\\\"op\\\":\\\"remove\\\",\\\"path\\\":\\\"\\\"}]}\"));\n\n  while (store.containsKey(\"road2\")) {\n    Thread.sleep(100);\n  }\n}",
                "start_point": {
                    "row": 129,
                    "column": 2
                },
                "end_point": {
                    "row": 139,
                    "column": 3
                }
            },
            {
                "definition": "@Test(timeout = 20000)\npublic void testMetrics() throws Exception {\n  RestTemplate restTemplate = new RestTemplate();\n  String fooResourceUrl = \"http://localhost:\" + port + \"/actuator/prometheus\";\n\n  Awaitility.await().atMost(5, SECONDS).pollInterval(100, MILLISECONDS).until(() -> {\n    ResponseEntity<String> response = restTemplate.getForEntity(fooResourceUrl, String.class);\n    assertThat(response.getStatusCode(), equalTo(HttpStatus.OK));\n    List<String> lines = Arrays.asList(response.getBody().split(\"\\n\")).stream().filter(l -> !l.startsWith(\"#\")).collect(toList());\n    assertThat(lines, not(emptyList()));\n  });\n}",
                "name": "testMetrics",
                "modifiers": "@Test(timeout = 20000)\n  public",
                "return_type": null,
                "parameters": [],
                "body": "{\n  RestTemplate restTemplate = new RestTemplate();\n  String fooResourceUrl = \"http://localhost:\" + port + \"/actuator/prometheus\";\n\n  Awaitility.await().atMost(5, SECONDS).pollInterval(100, MILLISECONDS).until(() -> {\n    ResponseEntity<String> response = restTemplate.getForEntity(fooResourceUrl, String.class);\n    assertThat(response.getStatusCode(), equalTo(HttpStatus.OK));\n    List<String> lines = Arrays.asList(response.getBody().split(\"\\n\")).stream().filter(l -> !l.startsWith(\"#\")).collect(toList());\n    assertThat(lines, not(emptyList()));\n  });\n}",
                "start_point": {
                    "row": 141,
                    "column": 2
                },
                "end_point": {
                    "row": 152,
                    "column": 3
                }
            },
            {
                "definition": "@AfterClass\npublic static void after() {\n  if (context != null) {\n    context.close();\n  }\n}",
                "name": "after",
                "modifiers": "@AfterClass\n  public static",
                "return_type": null,
                "parameters": [],
                "body": "{\n  if (context != null) {\n    context.close();\n  }\n}",
                "start_point": {
                    "row": 154,
                    "column": 2
                },
                "end_point": {
                    "row": 159,
                    "column": 3
                }
            }
        ]
    },
    {
        "rev_path": "monitoring/kafka-offset-metrics/src/main/java/com/hotels/road/kafka/offset/metrics/KafkaOffsetMetrics.java",
        "definition": "@Component\n@RequiredArgsConstructor\n@Slf4j\npublic class KafkaOffsetMetrics extends Collector {\n  private final AdminClient adminClient;\n  private final Supplier<String> hostnameSupplier;\n  private final CollectorRegistry collectorRegistry;\n\n  private static final List<String> LABELS =  Arrays.asList(\"host\", \"group\", \"topic\", \"partition\");\n\n  @PostConstruct\n  public void registerExporter() {\n    collectorRegistry.register(this);\n  }\n\n  @Override\n  public List<MetricFamilySamples> collect() {\n\n    List<Sample> samples = asJavaCollection(adminClient.listAllGroupsFlattened())\n        .stream()\n        .map(GroupOverview::groupId)\n        .flatMap(groupId ->\n        {\n          log.info(\"Found groupId: {}\", groupId);\n          return mapAsJavaMap(adminClient.listGroupOffsets(groupId))\n              .entrySet()\n              .stream()\n              .map(entry -> createSample(groupId, entry));\n        })\n        .collect(Collectors.toList());\n\n    MetricFamilySamples mfs = new MetricFamilySamples(\"kafka-offset\", Type.GAUGE, \"kafka-offset\", samples);\n\n    return Collections.singletonList(mfs);\n  }\n\n  private Sample createSample(String groupId, Map.Entry<TopicPartition, Object> entry) {\n    String partition = Integer.toString(entry.getKey().partition());\n    String topic = clean(entry.getKey().topic());\n\n    List<String> labelValues = Arrays.asList(hostnameSupplier.get(), clean(groupId), topic, partition);\n    Number offset = (Number) entry.getValue();\n\n    return new Sample(\"kafka-offset\", LABELS, labelValues, offset.doubleValue());\n  }\n\n  private String clean(String name) {\n    return name.replaceAll(\"\\\\.\", \"_\");\n  }\n}",
        "package": "package com.hotels.road.kafka.offset.metrics;",
        "tree_path": "KafkaOffsetMetrics",
        "name": "KafkaOffsetMetrics",
        "modifiers": "@Component\n@RequiredArgsConstructor\n@Slf4j\npublic",
        "superclass": "extends Collector",
        "super_interfaces": null,
        "body": "{\n  private final AdminClient adminClient;\n  private final Supplier<String> hostnameSupplier;\n  private final CollectorRegistry collectorRegistry;\n\n  private static final List<String> LABELS =  Arrays.asList(\"host\", \"group\", \"topic\", \"partition\");\n\n  @PostConstruct\n  public void registerExporter() {\n    collectorRegistry.register(this);\n  }\n\n  @Override\n  public List<MetricFamilySamples> collect() {\n\n    List<Sample> samples = asJavaCollection(adminClient.listAllGroupsFlattened())\n        .stream()\n        .map(GroupOverview::groupId)\n        .flatMap(groupId ->\n        {\n          log.info(\"Found groupId: {}\", groupId);\n          return mapAsJavaMap(adminClient.listGroupOffsets(groupId))\n              .entrySet()\n              .stream()\n              .map(entry -> createSample(groupId, entry));\n        })\n        .collect(Collectors.toList());\n\n    MetricFamilySamples mfs = new MetricFamilySamples(\"kafka-offset\", Type.GAUGE, \"kafka-offset\", samples);\n\n    return Collections.singletonList(mfs);\n  }\n\n  private Sample createSample(String groupId, Map.Entry<TopicPartition, Object> entry) {\n    String partition = Integer.toString(entry.getKey().partition());\n    String topic = clean(entry.getKey().topic());\n\n    List<String> labelValues = Arrays.asList(hostnameSupplier.get(), clean(groupId), topic, partition);\n    Number offset = (Number) entry.getValue();\n\n    return new Sample(\"kafka-offset\", LABELS, labelValues, offset.doubleValue());\n  }\n\n  private String clean(String name) {\n    return name.replaceAll(\"\\\\.\", \"_\");\n  }\n}",
        "start_point": {
            "row": 39,
            "column": 0
        },
        "end_point": {
            "row": 88,
            "column": 1
        },
        "file_mode": "Modified",
        "map_path": null,
        "methods": [
            {
                "definition": "@PostConstruct\npublic void registerExporter() {\n  collectorRegistry.register(this);\n}",
                "name": "registerExporter",
                "modifiers": "@PostConstruct\n  public",
                "return_type": null,
                "parameters": [],
                "body": "{\n  collectorRegistry.register(this);\n}",
                "start_point": {
                    "row": 49,
                    "column": 2
                },
                "end_point": {
                    "row": 52,
                    "column": 3
                }
            },
            {
                "definition": "@Override\npublic List<MetricFamilySamples> collect() {\n\n  List<Sample> samples = asJavaCollection(adminClient.listAllGroupsFlattened())\n      .stream()\n      .map(GroupOverview::groupId)\n      .flatMap(groupId ->\n      {\n        log.info(\"Found groupId: {}\", groupId);\n        return mapAsJavaMap(adminClient.listGroupOffsets(groupId))\n            .entrySet()\n            .stream()\n            .map(entry -> createSample(groupId, entry));\n      })\n      .collect(Collectors.toList());\n\n  MetricFamilySamples mfs = new MetricFamilySamples(\"kafka-offset\", Type.GAUGE, \"kafka-offset\", samples);\n\n  return Collections.singletonList(mfs);\n}",
                "name": "collect",
                "modifiers": "@Override\n  public",
                "return_type": null,
                "parameters": [],
                "body": "{\n\n  List<Sample> samples = asJavaCollection(adminClient.listAllGroupsFlattened())\n      .stream()\n      .map(GroupOverview::groupId)\n      .flatMap(groupId ->\n      {\n        log.info(\"Found groupId: {}\", groupId);\n        return mapAsJavaMap(adminClient.listGroupOffsets(groupId))\n            .entrySet()\n            .stream()\n            .map(entry -> createSample(groupId, entry));\n      })\n      .collect(Collectors.toList());\n\n  MetricFamilySamples mfs = new MetricFamilySamples(\"kafka-offset\", Type.GAUGE, \"kafka-offset\", samples);\n\n  return Collections.singletonList(mfs);\n}",
                "start_point": {
                    "row": 54,
                    "column": 2
                },
                "end_point": {
                    "row": 73,
                    "column": 3
                }
            },
            {
                "definition": "private Sample createSample(String groupId, Map.Entry<TopicPartition, Object> entry) {\n  String partition = Integer.toString(entry.getKey().partition());\n  String topic = clean(entry.getKey().topic());\n\n  List<String> labelValues = Arrays.asList(hostnameSupplier.get(), clean(groupId), topic, partition);\n  Number offset = (Number) entry.getValue();\n\n  return new Sample(\"kafka-offset\", LABELS, labelValues, offset.doubleValue());\n}",
                "name": "createSample",
                "modifiers": "private",
                "return_type": "Sample",
                "parameters": [
                    {
                        "type": "String",
                        "name": "groupId"
                    },
                    {
                        "type": "Map.Entry<TopicPartition, Object>",
                        "name": "entry"
                    }
                ],
                "body": "{\n  String partition = Integer.toString(entry.getKey().partition());\n  String topic = clean(entry.getKey().topic());\n\n  List<String> labelValues = Arrays.asList(hostnameSupplier.get(), clean(groupId), topic, partition);\n  Number offset = (Number) entry.getValue();\n\n  return new Sample(\"kafka-offset\", LABELS, labelValues, offset.doubleValue());\n}",
                "start_point": {
                    "row": 75,
                    "column": 2
                },
                "end_point": {
                    "row": 83,
                    "column": 3
                }
            },
            {
                "definition": "private String clean(String name) {\n  return name.replaceAll(\"\\\\.\", \"_\");\n}",
                "name": "clean",
                "modifiers": "private",
                "return_type": "String",
                "parameters": [
                    {
                        "type": "String",
                        "name": "name"
                    }
                ],
                "body": "{\n  return name.replaceAll(\"\\\\.\", \"_\");\n}",
                "start_point": {
                    "row": 85,
                    "column": 2
                },
                "end_point": {
                    "row": 87,
                    "column": 3
                }
            }
        ]
    },
    {
        "rev_path": "monitoring/kafka-offset-metrics/src/main/java/com/hotels/road/kafka/offset/metrics/KafkaOffsetMetricsApp.java",
        "definition": "@SpringBootApplication\n@EnableScheduling\npublic class KafkaOffsetMetricsApp {\n  @Bean\n  AdminClient adminClient(@Value(\"${kafka.bootstrapServers}\") String bootstrapServers) {\n    Properties properties = new Properties();\n    properties.setProperty(\"bootstrap.servers\", bootstrapServers);\n    return AdminClient.create(properties);\n  }\n\n  @Bean\n  Supplier<String> hostnameSupplier() {\n    return () -> {\n      try {\n        return InetAddress.getLocalHost().getHostName();\n      } catch (UnknownHostException e) {\n        throw new RuntimeException(e);\n      }\n    };\n  }\n\n  public static void main(String[] args) {\n    DataHighwayApplication.run(KafkaOffsetMetricsApp.class, args);\n  }\n}",
        "package": "package com.hotels.road.kafka.offset.metrics;",
        "tree_path": "KafkaOffsetMetricsApp",
        "name": "KafkaOffsetMetricsApp",
        "modifiers": "@SpringBootApplication\n@EnableScheduling\npublic",
        "superclass": null,
        "super_interfaces": null,
        "body": "{\n  @Bean\n  AdminClient adminClient(@Value(\"${kafka.bootstrapServers}\") String bootstrapServers) {\n    Properties properties = new Properties();\n    properties.setProperty(\"bootstrap.servers\", bootstrapServers);\n    return AdminClient.create(properties);\n  }\n\n  @Bean\n  Supplier<String> hostnameSupplier() {\n    return () -> {\n      try {\n        return InetAddress.getLocalHost().getHostName();\n      } catch (UnknownHostException e) {\n        throw new RuntimeException(e);\n      }\n    };\n  }\n\n  public static void main(String[] args) {\n    DataHighwayApplication.run(KafkaOffsetMetricsApp.class, args);\n  }\n}",
        "start_point": {
            "row": 31,
            "column": 0
        },
        "end_point": {
            "row": 55,
            "column": 1
        },
        "file_mode": "Modified",
        "map_path": null,
        "methods": [
            {
                "definition": "@Bean\nAdminClient adminClient(@Value(\"${kafka.bootstrapServers}\") String bootstrapServers) {\n  Properties properties = new Properties();\n  properties.setProperty(\"bootstrap.servers\", bootstrapServers);\n  return AdminClient.create(properties);\n}",
                "name": "adminClient",
                "modifiers": "@Bean",
                "return_type": "AdminClient",
                "parameters": [
                    {
                        "type": "@Value(\"${kafka.bootstrapServers}\")",
                        "name": "String"
                    }
                ],
                "body": "{\n  Properties properties = new Properties();\n  properties.setProperty(\"bootstrap.servers\", bootstrapServers);\n  return AdminClient.create(properties);\n}",
                "start_point": {
                    "row": 34,
                    "column": 2
                },
                "end_point": {
                    "row": 39,
                    "column": 3
                }
            },
            {
                "definition": "@Bean\nSupplier<String> hostnameSupplier() {\n  return () -> {\n    try {\n      return InetAddress.getLocalHost().getHostName();\n    } catch (UnknownHostException e) {\n      throw new RuntimeException(e);\n    }\n  };\n}",
                "name": "hostnameSupplier",
                "modifiers": "@Bean",
                "return_type": null,
                "parameters": [],
                "body": "{\n  return () -> {\n    try {\n      return InetAddress.getLocalHost().getHostName();\n    } catch (UnknownHostException e) {\n      throw new RuntimeException(e);\n    }\n  };\n}",
                "start_point": {
                    "row": 41,
                    "column": 2
                },
                "end_point": {
                    "row": 50,
                    "column": 3
                }
            },
            {
                "definition": "public static void main(String[] args) {\n  DataHighwayApplication.run(KafkaOffsetMetricsApp.class, args);\n}",
                "name": "main",
                "modifiers": "public static",
                "return_type": null,
                "parameters": [
                    {
                        "type": "String[]",
                        "name": "args"
                    }
                ],
                "body": "{\n  DataHighwayApplication.run(KafkaOffsetMetricsApp.class, args);\n}",
                "start_point": {
                    "row": 52,
                    "column": 2
                },
                "end_point": {
                    "row": 54,
                    "column": 3
                }
            }
        ]
    },
    {
        "rev_path": "monitoring/kafka-offset-metrics/src/test/java/com/hotels/road/kafka/offset/metrics/KafkaOffsetMetricsAppTest.java",
        "definition": "public class KafkaOffsetMetricsAppTest {\n\n  private static final String TOPIC = \"test.topic\";\n\n  @Rule\n  public EmbeddedKafkaCluster kafka = new EmbeddedKafkaCluster(1);\n\n  @Test\n  public void test() throws Exception {\n    kafka.createTopic(TOPIC);\n\n    try (KafkaConsumer<String, String> consumer = consumer()) {\n      consumer.commitSync(singletonMap(new TopicPartition(TOPIC, 0), new OffsetAndMetadata(1L)));\n    }\n\n    RestTemplate restTemplate = new RestTemplate();\n    String fooResourceUrl = \"http://localhost:8080/actuator/prometheus\";\n\n    try (ConfigurableApplicationContext context = runApp()) {\n      Awaitility.await().atMost(5, SECONDS).pollInterval(100, MILLISECONDS).until(() -> {\n        ResponseEntity<String> response = restTemplate.getForEntity(fooResourceUrl, String.class);\n        assertThat(response.getStatusCode(), equalTo(HttpStatus.OK));\n        List<String> lines = Arrays.asList(response.getBody().split(\"\\n\")).stream().filter(l -> l.startsWith(\"kafka\")).collect(toList());\n\n        //Should contain one metric in the following format\n        // kafka-offset{host=\"<hostname>\",group=\"group_id\",topic=\"test_topic\",partition=\"0\",} 1.0\n        assertThat(lines, hasSize(1));\n        assertThat(lines.get(0), startsWith(\"kafka-offset\"));\n        assertThat(lines.get(0), containsString(\"group=\\\"group_id\\\"\"));\n        assertThat(lines.get(0), containsString(\"topic=\\\"test_topic\\\"\"));\n        assertThat(lines.get(0), containsString(\"partition=\\\"0\\\"\"));\n        assertThat(lines.get(0), endsWith(\" 1.0\"));\n      });\n    }\n  }\n\n  private ConfigurableApplicationContext runApp() {\n    String[] args = ImmutableMap\n        .<String, String> builder()\n        .put(\"kafka.bootstrapServers\", kafka.bootstrapServers())\n        .build()\n        .entrySet()\n        .stream()\n        .map(e -> String.format(\"--%s=%s\", e.getKey(), e.getValue()))\n        .toArray(i -> new String[i]);\n    return new SpringApplicationBuilder(KafkaOffsetMetricsApp.class).bannerMode(OFF).run(args);\n  }\n\n  private KafkaConsumer<String, String> consumer() {\n    Map<String, Object> config = ImmutableMap\n        .<String, Object> builder()\n        .put(\"bootstrap.servers\", kafka.bootstrapServers())\n        .put(\"group.id\", \"group.id\")\n        .put(\"enable.auto.commit\", \"false\")\n        .put(\"auto.offset.reset\", \"earliest\")\n        .build();\n    Deserializer<String> deserializer = new StringDeserializer();\n    return new KafkaConsumer<>(config, deserializer, deserializer);\n  }\n}",
        "package": "package com.hotels.road.kafka.offset.metrics;",
        "tree_path": "KafkaOffsetMetricsAppTest",
        "name": "KafkaOffsetMetricsAppTest",
        "modifiers": "public",
        "superclass": null,
        "super_interfaces": null,
        "body": "{\n\n  private static final String TOPIC = \"test.topic\";\n\n  @Rule\n  public EmbeddedKafkaCluster kafka = new EmbeddedKafkaCluster(1);\n\n  @Test\n  public void test() throws Exception {\n    kafka.createTopic(TOPIC);\n\n    try (KafkaConsumer<String, String> consumer = consumer()) {\n      consumer.commitSync(singletonMap(new TopicPartition(TOPIC, 0), new OffsetAndMetadata(1L)));\n    }\n\n    RestTemplate restTemplate = new RestTemplate();\n    String fooResourceUrl = \"http://localhost:8080/actuator/prometheus\";\n\n    try (ConfigurableApplicationContext context = runApp()) {\n      Awaitility.await().atMost(5, SECONDS).pollInterval(100, MILLISECONDS).until(() -> {\n        ResponseEntity<String> response = restTemplate.getForEntity(fooResourceUrl, String.class);\n        assertThat(response.getStatusCode(), equalTo(HttpStatus.OK));\n        List<String> lines = Arrays.asList(response.getBody().split(\"\\n\")).stream().filter(l -> l.startsWith(\"kafka\")).collect(toList());\n\n        //Should contain one metric in the following format\n        // kafka-offset{host=\"<hostname>\",group=\"group_id\",topic=\"test_topic\",partition=\"0\",} 1.0\n        assertThat(lines, hasSize(1));\n        assertThat(lines.get(0), startsWith(\"kafka-offset\"));\n        assertThat(lines.get(0), containsString(\"group=\\\"group_id\\\"\"));\n        assertThat(lines.get(0), containsString(\"topic=\\\"test_topic\\\"\"));\n        assertThat(lines.get(0), containsString(\"partition=\\\"0\\\"\"));\n        assertThat(lines.get(0), endsWith(\" 1.0\"));\n      });\n    }\n  }\n\n  private ConfigurableApplicationContext runApp() {\n    String[] args = ImmutableMap\n        .<String, String> builder()\n        .put(\"kafka.bootstrapServers\", kafka.bootstrapServers())\n        .build()\n        .entrySet()\n        .stream()\n        .map(e -> String.format(\"--%s=%s\", e.getKey(), e.getValue()))\n        .toArray(i -> new String[i]);\n    return new SpringApplicationBuilder(KafkaOffsetMetricsApp.class).bannerMode(OFF).run(args);\n  }\n\n  private KafkaConsumer<String, String> consumer() {\n    Map<String, Object> config = ImmutableMap\n        .<String, Object> builder()\n        .put(\"bootstrap.servers\", kafka.bootstrapServers())\n        .put(\"group.id\", \"group.id\")\n        .put(\"enable.auto.commit\", \"false\")\n        .put(\"auto.offset.reset\", \"earliest\")\n        .build();\n    Deserializer<String> deserializer = new StringDeserializer();\n    return new KafkaConsumer<>(config, deserializer, deserializer);\n  }\n}",
        "start_point": {
            "row": 51,
            "column": 0
        },
        "end_point": {
            "row": 110,
            "column": 1
        },
        "file_mode": "Modified",
        "map_path": null,
        "methods": [
            {
                "definition": "@Test\npublic void test() throws Exception {\n  kafka.createTopic(TOPIC);\n\n  try (KafkaConsumer<String, String> consumer = consumer()) {\n    consumer.commitSync(singletonMap(new TopicPartition(TOPIC, 0), new OffsetAndMetadata(1L)));\n  }\n\n  RestTemplate restTemplate = new RestTemplate();\n  String fooResourceUrl = \"http://localhost:8080/actuator/prometheus\";\n\n  try (ConfigurableApplicationContext context = runApp()) {\n    Awaitility.await().atMost(5, SECONDS).pollInterval(100, MILLISECONDS).until(() -> {\n      ResponseEntity<String> response = restTemplate.getForEntity(fooResourceUrl, String.class);\n      assertThat(response.getStatusCode(), equalTo(HttpStatus.OK));\n      List<String> lines = Arrays.asList(response.getBody().split(\"\\n\")).stream().filter(l -> l.startsWith(\"kafka\")).collect(toList());\n\n      //Should contain one metric in the following format\n      // kafka-offset{host=\"<hostname>\",group=\"group_id\",topic=\"test_topic\",partition=\"0\",} 1.0\n      assertThat(lines, hasSize(1));\n      assertThat(lines.get(0), startsWith(\"kafka-offset\"));\n      assertThat(lines.get(0), containsString(\"group=\\\"group_id\\\"\"));\n      assertThat(lines.get(0), containsString(\"topic=\\\"test_topic\\\"\"));\n      assertThat(lines.get(0), containsString(\"partition=\\\"0\\\"\"));\n      assertThat(lines.get(0), endsWith(\" 1.0\"));\n    });\n  }\n}",
                "name": "test",
                "modifiers": "@Test\n  public",
                "return_type": null,
                "parameters": [],
                "body": "{\n  kafka.createTopic(TOPIC);\n\n  try (KafkaConsumer<String, String> consumer = consumer()) {\n    consumer.commitSync(singletonMap(new TopicPartition(TOPIC, 0), new OffsetAndMetadata(1L)));\n  }\n\n  RestTemplate restTemplate = new RestTemplate();\n  String fooResourceUrl = \"http://localhost:8080/actuator/prometheus\";\n\n  try (ConfigurableApplicationContext context = runApp()) {\n    Awaitility.await().atMost(5, SECONDS).pollInterval(100, MILLISECONDS).until(() -> {\n      ResponseEntity<String> response = restTemplate.getForEntity(fooResourceUrl, String.class);\n      assertThat(response.getStatusCode(), equalTo(HttpStatus.OK));\n      List<String> lines = Arrays.asList(response.getBody().split(\"\\n\")).stream().filter(l -> l.startsWith(\"kafka\")).collect(toList());\n\n      //Should contain one metric in the following format\n      // kafka-offset{host=\"<hostname>\",group=\"group_id\",topic=\"test_topic\",partition=\"0\",} 1.0\n      assertThat(lines, hasSize(1));\n      assertThat(lines.get(0), startsWith(\"kafka-offset\"));\n      assertThat(lines.get(0), containsString(\"group=\\\"group_id\\\"\"));\n      assertThat(lines.get(0), containsString(\"topic=\\\"test_topic\\\"\"));\n      assertThat(lines.get(0), containsString(\"partition=\\\"0\\\"\"));\n      assertThat(lines.get(0), endsWith(\" 1.0\"));\n    });\n  }\n}",
                "start_point": {
                    "row": 58,
                    "column": 2
                },
                "end_point": {
                    "row": 85,
                    "column": 3
                }
            },
            {
                "definition": "private ConfigurableApplicationContext runApp() {\n  String[] args = ImmutableMap\n      .<String, String> builder()\n      .put(\"kafka.bootstrapServers\", kafka.bootstrapServers())\n      .build()\n      .entrySet()\n      .stream()\n      .map(e -> String.format(\"--%s=%s\", e.getKey(), e.getValue()))\n      .toArray(i -> new String[i]);\n  return new SpringApplicationBuilder(KafkaOffsetMetricsApp.class).bannerMode(OFF).run(args);\n}",
                "name": "runApp",
                "modifiers": "private",
                "return_type": "ConfigurableApplicationContext",
                "parameters": [],
                "body": "{\n  String[] args = ImmutableMap\n      .<String, String> builder()\n      .put(\"kafka.bootstrapServers\", kafka.bootstrapServers())\n      .build()\n      .entrySet()\n      .stream()\n      .map(e -> String.format(\"--%s=%s\", e.getKey(), e.getValue()))\n      .toArray(i -> new String[i]);\n  return new SpringApplicationBuilder(KafkaOffsetMetricsApp.class).bannerMode(OFF).run(args);\n}",
                "start_point": {
                    "row": 87,
                    "column": 2
                },
                "end_point": {
                    "row": 97,
                    "column": 3
                }
            },
            {
                "definition": "private KafkaConsumer<String, String> consumer() {\n  Map<String, Object> config = ImmutableMap\n      .<String, Object> builder()\n      .put(\"bootstrap.servers\", kafka.bootstrapServers())\n      .put(\"group.id\", \"group.id\")\n      .put(\"enable.auto.commit\", \"false\")\n      .put(\"auto.offset.reset\", \"earliest\")\n      .build();\n  Deserializer<String> deserializer = new StringDeserializer();\n  return new KafkaConsumer<>(config, deserializer, deserializer);\n}",
                "name": "consumer",
                "modifiers": "private",
                "return_type": null,
                "parameters": [],
                "body": "{\n  Map<String, Object> config = ImmutableMap\n      .<String, Object> builder()\n      .put(\"bootstrap.servers\", kafka.bootstrapServers())\n      .put(\"group.id\", \"group.id\")\n      .put(\"enable.auto.commit\", \"false\")\n      .put(\"auto.offset.reset\", \"earliest\")\n      .build();\n  Deserializer<String> deserializer = new StringDeserializer();\n  return new KafkaConsumer<>(config, deserializer, deserializer);\n}",
                "start_point": {
                    "row": 99,
                    "column": 2
                },
                "end_point": {
                    "row": 109,
                    "column": 3
                }
            }
        ]
    },
    {
        "rev_path": "monitoring/kafka-offset-metrics/src/test/java/com/hotels/road/kafka/offset/metrics/KafkaOffsetMetricsTest.java",
        "definition": "@RunWith(MockitoJUnitRunner.class)\npublic class KafkaOffsetMetricsTest {\n\n  @Mock\n  private AdminClient adminClient;\n\n  @Mock\n  private Supplier<String> hostnameSupplier;\n\n  @Mock\n  private CollectorRegistry collectorRegistry;\n\n  private KafkaOffsetMetrics underTest;\n\n  @Before\n  public void before() {\n    underTest = new KafkaOffsetMetrics(adminClient, hostnameSupplier, collectorRegistry);\n  }\n\n  @SuppressWarnings({ \"rawtypes\", \"unchecked\" })\n  @Test\n  public void happyPath() {\n    scala.collection.immutable.List<GroupOverview> listAllGroupsFlattened = asScalaSet(\n        singleton(GroupOverview.apply(\"groupId\", \"\"))).toList();\n    when(adminClient.listAllGroupsFlattened()).thenReturn(listAllGroupsFlattened);\n\n    scala.collection.immutable.Map<TopicPartition, Object> offsets = asScalaMap(\n        singletonMap(new TopicPartition(\"topicName\", 0), (Object) 1L));\n    when(adminClient.listGroupOffsets(\"groupId\")).thenReturn(offsets);\n\n    when(hostnameSupplier.get()).thenReturn(\"localhost\");\n\n    List<Collector.MetricFamilySamples> collection = underTest.collect();\n\n    assertThat(collection.size(), is(1));\n    Collector.MetricFamilySamples mfs = collection.get(0);\n\n    assertThat(mfs.name, is(\"kafka-offset\"));\n    assertThat(mfs.type, is(GAUGE));\n    assertThat(mfs.samples, is(not(empty())));\n    assertThat(mfs.samples, is(hasSize(1)));\n\n    Collector.MetricFamilySamples.Sample sample = mfs.samples.get(0);\n\n    assertThat(sample.name, is(\"kafka-offset\"));\n    assertThat(sample.labelNames, is(asList(\"host\", \"group\", \"topic\", \"partition\")));\n    assertThat(sample.labelValues, is(asList(\"localhost\", \"groupId\", \"topicName\", \"0\")));\n    assertThat(sample.value, is(1.0d));\n  }\n\n  private scala.collection.immutable.Map<TopicPartition, Object> asScalaMap(Map<TopicPartition, Object> map) {\n    return mapAsScalaMap(map).toMap(Predef.<Tuple2<TopicPartition, Object>> conforms());\n  }\n}",
        "package": "package com.hotels.road.kafka.offset.metrics;",
        "tree_path": "KafkaOffsetMetricsTest",
        "name": "KafkaOffsetMetricsTest",
        "modifiers": "@RunWith(MockitoJUnitRunner.class)\npublic",
        "superclass": null,
        "super_interfaces": null,
        "body": "{\n\n  @Mock\n  private AdminClient adminClient;\n\n  @Mock\n  private Supplier<String> hostnameSupplier;\n\n  @Mock\n  private CollectorRegistry collectorRegistry;\n\n  private KafkaOffsetMetrics underTest;\n\n  @Before\n  public void before() {\n    underTest = new KafkaOffsetMetrics(adminClient, hostnameSupplier, collectorRegistry);\n  }\n\n  @SuppressWarnings({ \"rawtypes\", \"unchecked\" })\n  @Test\n  public void happyPath() {\n    scala.collection.immutable.List<GroupOverview> listAllGroupsFlattened = asScalaSet(\n        singleton(GroupOverview.apply(\"groupId\", \"\"))).toList();\n    when(adminClient.listAllGroupsFlattened()).thenReturn(listAllGroupsFlattened);\n\n    scala.collection.immutable.Map<TopicPartition, Object> offsets = asScalaMap(\n        singletonMap(new TopicPartition(\"topicName\", 0), (Object) 1L));\n    when(adminClient.listGroupOffsets(\"groupId\")).thenReturn(offsets);\n\n    when(hostnameSupplier.get()).thenReturn(\"localhost\");\n\n    List<Collector.MetricFamilySamples> collection = underTest.collect();\n\n    assertThat(collection.size(), is(1));\n    Collector.MetricFamilySamples mfs = collection.get(0);\n\n    assertThat(mfs.name, is(\"kafka-offset\"));\n    assertThat(mfs.type, is(GAUGE));\n    assertThat(mfs.samples, is(not(empty())));\n    assertThat(mfs.samples, is(hasSize(1)));\n\n    Collector.MetricFamilySamples.Sample sample = mfs.samples.get(0);\n\n    assertThat(sample.name, is(\"kafka-offset\"));\n    assertThat(sample.labelNames, is(asList(\"host\", \"group\", \"topic\", \"partition\")));\n    assertThat(sample.labelValues, is(asList(\"localhost\", \"groupId\", \"topicName\", \"0\")));\n    assertThat(sample.value, is(1.0d));\n  }\n\n  private scala.collection.immutable.Map<TopicPartition, Object> asScalaMap(Map<TopicPartition, Object> map) {\n    return mapAsScalaMap(map).toMap(Predef.<Tuple2<TopicPartition, Object>> conforms());\n  }\n}",
        "start_point": {
            "row": 50,
            "column": 0
        },
        "end_point": {
            "row": 103,
            "column": 1
        },
        "file_mode": "Modified",
        "map_path": null,
        "methods": [
            {
                "definition": "@Before\npublic void before() {\n  underTest = new KafkaOffsetMetrics(adminClient, hostnameSupplier, collectorRegistry);\n}",
                "name": "before",
                "modifiers": "@Before\n  public",
                "return_type": null,
                "parameters": [],
                "body": "{\n  underTest = new KafkaOffsetMetrics(adminClient, hostnameSupplier, collectorRegistry);\n}",
                "start_point": {
                    "row": 64,
                    "column": 2
                },
                "end_point": {
                    "row": 67,
                    "column": 3
                }
            },
            {
                "definition": "@SuppressWarnings({ \"rawtypes\", \"unchecked\" })\n@Test\npublic void happyPath() {\n  scala.collection.immutable.List<GroupOverview> listAllGroupsFlattened = asScalaSet(\n      singleton(GroupOverview.apply(\"groupId\", \"\"))).toList();\n  when(adminClient.listAllGroupsFlattened()).thenReturn(listAllGroupsFlattened);\n\n  scala.collection.immutable.Map<TopicPartition, Object> offsets = asScalaMap(\n      singletonMap(new TopicPartition(\"topicName\", 0), (Object) 1L));\n  when(adminClient.listGroupOffsets(\"groupId\")).thenReturn(offsets);\n\n  when(hostnameSupplier.get()).thenReturn(\"localhost\");\n\n  List<Collector.MetricFamilySamples> collection = underTest.collect();\n\n  assertThat(collection.size(), is(1));\n  Collector.MetricFamilySamples mfs = collection.get(0);\n\n  assertThat(mfs.name, is(\"kafka-offset\"));\n  assertThat(mfs.type, is(GAUGE));\n  assertThat(mfs.samples, is(not(empty())));\n  assertThat(mfs.samples, is(hasSize(1)));\n\n  Collector.MetricFamilySamples.Sample sample = mfs.samples.get(0);\n\n  assertThat(sample.name, is(\"kafka-offset\"));\n  assertThat(sample.labelNames, is(asList(\"host\", \"group\", \"topic\", \"partition\")));\n  assertThat(sample.labelValues, is(asList(\"localhost\", \"groupId\", \"topicName\", \"0\")));\n  assertThat(sample.value, is(1.0d));\n}",
                "name": "happyPath",
                "modifiers": "@SuppressWarnings({ \"rawtypes\", \"unchecked\" })\n  @Test\n  public",
                "return_type": null,
                "parameters": [],
                "body": "{\n  scala.collection.immutable.List<GroupOverview> listAllGroupsFlattened = asScalaSet(\n      singleton(GroupOverview.apply(\"groupId\", \"\"))).toList();\n  when(adminClient.listAllGroupsFlattened()).thenReturn(listAllGroupsFlattened);\n\n  scala.collection.immutable.Map<TopicPartition, Object> offsets = asScalaMap(\n      singletonMap(new TopicPartition(\"topicName\", 0), (Object) 1L));\n  when(adminClient.listGroupOffsets(\"groupId\")).thenReturn(offsets);\n\n  when(hostnameSupplier.get()).thenReturn(\"localhost\");\n\n  List<Collector.MetricFamilySamples> collection = underTest.collect();\n\n  assertThat(collection.size(), is(1));\n  Collector.MetricFamilySamples mfs = collection.get(0);\n\n  assertThat(mfs.name, is(\"kafka-offset\"));\n  assertThat(mfs.type, is(GAUGE));\n  assertThat(mfs.samples, is(not(empty())));\n  assertThat(mfs.samples, is(hasSize(1)));\n\n  Collector.MetricFamilySamples.Sample sample = mfs.samples.get(0);\n\n  assertThat(sample.name, is(\"kafka-offset\"));\n  assertThat(sample.labelNames, is(asList(\"host\", \"group\", \"topic\", \"partition\")));\n  assertThat(sample.labelValues, is(asList(\"localhost\", \"groupId\", \"topicName\", \"0\")));\n  assertThat(sample.value, is(1.0d));\n}",
                "start_point": {
                    "row": 69,
                    "column": 2
                },
                "end_point": {
                    "row": 98,
                    "column": 3
                }
            },
            {
                "definition": "private scala.collection.immutable.Map<TopicPartition, Object> asScalaMap(Map<TopicPartition, Object> map) {\n  return mapAsScalaMap(map).toMap(Predef.<Tuple2<TopicPartition, Object>> conforms());\n}",
                "name": "asScalaMap",
                "modifiers": "private",
                "return_type": null,
                "parameters": [
                    {
                        "type": "Map<TopicPartition, Object>",
                        "name": "map"
                    }
                ],
                "body": "{\n  return mapAsScalaMap(map).toMap(Predef.<Tuple2<TopicPartition, Object>> conforms());\n}",
                "start_point": {
                    "row": 100,
                    "column": 2
                },
                "end_point": {
                    "row": 102,
                    "column": 3
                }
            }
        ]
    }
]